---
title: "Análisis Exploratorio de Datos - Tweets con Toxicidad"
author: "Análisis de Machine Learning"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
    theme: cosmo
jupyter: python3
---

# Introducción


```{python}
#| label: setup
#| echo: false
#| warning: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from scipy.stats import skew, kurtosis
from scipy import stats
from collections import Counter
import re
import warnings
from nltk.corpus import stopwords
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

df = pd.read_csv('/home/juanchx/Documents/Maestria_IA/machine learning/final_examen_data/1500_tweets_con_toxicity.csv')
```

Dataset cargado: **1,500** registros x **{python} df.shape[1]** columnas

# 1. Exploración Inicial de Datos

## 1.1 Tipos de Variables

```{python}
#| label: tipos-datos

numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
boolean_cols = df.select_dtypes(include=['bool']).columns.tolist()

print(f"Variables numéricas: {len(numeric_cols)}")
print(f"Variables categóricas: {len(categorical_cols)}")
print(f"Variables booleanas: {len(boolean_cols)}")
```

## 1.2 Valores Nulos y Duplicados

```{python}
#| label: valores-nulos

null_counts = df.isnull().sum()
null_percentages = (null_counts / len(df)) * 100
null_df = pd.DataFrame({'Columna': null_counts.index, 'Nulos': null_counts.values, 
                        'Porcentaje': null_percentages.values}).sort_values('Nulos', ascending=False)

if null_df['Nulos'].sum() > 0:
    fig, ax = plt.subplots(figsize=(12, 6))
    null_data = null_df[null_df['Nulos'] > 0]
    ax.barh(null_data['Columna'], null_data['Porcentaje'], color='coral')
    ax.set_xlabel('Porcentaje de Valores Nulos (%)')
    ax.set_title('Valores Nulos por Columna', fontweight='bold')
    plt.tight_layout()
    plt.show()
else:
    print("✓ No hay valores nulos")

duplicates = df.duplicated().sum()
print(f"Duplicados: {duplicates}")
```

## 1.3 Distribución de Toxicidad

```{python}
#| label: dist-toxicity
#| fig-cap: "Distribución del score de toxicidad"

toxicity_data = df['toxicity_score'].dropna()
print("\nESTADÍSTICAS:\n", toxicity_data.describe())

print(f"\nAsimetría: {skew(toxicity_data):.4f}")
print(f"Curtosis: {kurtosis(toxicity_data):.4f}")

Q1, Q3 = toxicity_data.quantile([0.25, 0.75])
IQR = Q3 - Q1
outliers = toxicity_data[(toxicity_data < Q1 - 1.5*IQR) | (toxicity_data > Q3 + 1.5*IQR)]
print(f"\nOUTLIERS (IQR): {len(outliers)} ({(len(outliers)/len(toxicity_data))*100:.2f}%)")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

axes[0, 0].hist(toxicity_data, bins=50, edgecolor='black', alpha=0.7, density=True)
toxicity_data.plot(kind='kde', ax=axes[0, 0], color='red', linewidth=2)
axes[0, 0].axvline(toxicity_data.mean(), color='green', linestyle='--', label=f'Media: {toxicity_data.mean():.3f}')
axes[0, 0].axvline(toxicity_data.median(), color='orange', linestyle='--', label=f'Mediana: {toxicity_data.median():.3f}')
axes[0, 0].set_title('Distribución Toxicity Score', fontweight='bold')
axes[0, 0].legend()

axes[0, 1].boxplot(toxicity_data)
axes[0, 1].set_title('Boxplot - Outliers', fontweight='bold')

stats.probplot(toxicity_data, dist="norm", plot=axes[1, 0])
axes[1, 0].set_title('Q-Q Plot', fontweight='bold')

axes[1, 1].violinplot(toxicity_data, showmeans=True, showmedians=True)
axes[1, 1].set_title('Violin Plot', fontweight='bold')

plt.tight_layout()
plt.show()
```

## 1.4 Variables Numéricas

```{python}
#| label: vars-numericas

# Variables numéricas seleccionadas para análisis
# Excluidas: 
#   - hashtags_count: todos los valores son cero (sin varianza)
#   - sentiment_polarity: variable dummy mal obtenida desde el origen (no aporta información)
#   - IDs (tweetId, authorId, conversationId, inReplyToId): alta cardinalidad, no aportan información
numeric_features = ['authorFollowers', 'time_response', 'account_age_days', 
                   'mentions_count', 'content_length']

print("\nESTADÍSTICAS:\n", df[numeric_features].describe().T)

correlations = df[numeric_features + ['toxicity_score']].corr()['toxicity_score'].drop('toxicity_score').sort_values(ascending=False)
print("\nCORRELACIÓN CON toxicity_score:\n", correlations)

fig, ax = plt.subplots(figsize=(10, 6))
correlations.plot(kind='barh', ax=ax, color=['green' if x > 0 else 'red' for x in correlations])
ax.set_xlabel('Correlación con toxicity_score')
ax.set_title('Correlaciones', fontweight='bold')
ax.axvline(0, color='black', linewidth=0.8)
plt.tight_layout()
# plt.savefig('03_correlaciones.png', dpi=300)
# print("✓ Guardado: 03_correlaciones.png")
plt.show()

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()
for idx, col in enumerate(numeric_features):
    axes[idx].hist(df[col], bins=50, edgecolor='black', alpha=0.7)
    axes[idx].set_title(col, fontweight='bold')
    axes[idx].axvline(df[col].mean(), color='red', linestyle='--', label='Media')
    axes[idx].legend(fontsize=8)
# Eliminar el subplot extra
fig.delaxes(axes[5])
plt.tight_layout()
# plt.savefig('04_histogramas_numericas.png', dpi=300)
# print("✓ Guardado: 04_histogramas_numericas.png")
plt.show()

df_clean = df.dropna(subset=['toxicity_score'])
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()
for idx, col in enumerate(numeric_features):
    axes[idx].scatter(df_clean[col], df_clean['toxicity_score'], alpha=0.5, s=20)
    axes[idx].set_xlabel(col)
    axes[idx].set_ylabel('toxicity_score')
    axes[idx].set_title(f'{col} vs Toxicity', fontweight='bold')
    # Solo hacer línea de tendencia si hay varianza
    if df_clean[col].std() > 0:
        try:
            z = np.polyfit(df_clean[col], df_clean['toxicity_score'], 1)
            p = np.poly1d(z)
            axes[idx].plot(df_clean[col], p(df_clean[col]), "r--", alpha=0.8)
        except:
            pass
# Eliminar el subplot extra
fig.delaxes(axes[5])
plt.tight_layout()
# plt.savefig('05_scatterplots_toxicity.png', dpi=300)
# print("✓ Guardado: 05_scatterplots_toxicity.png")
plt.show()
```

## 1.5 Variables Categóricas

```{python}
#| label: vars-categoricas

# Variables categóricas seleccionadas para análisis
# Excluidas:
#   - isReply: todos los valores son True (sin varianza)
#   - authorVerified: todos los valores son False (sin varianza)
#   - source: todos los valores son Twitter for iPhone (sin varianza)
#   - hashtags: 92% de valores nulos
#   - authorName, authorUsername: alta cardinalidad, no aportan información predictiva
categorical_features = ['has_profile_picture']

for col in categorical_features:
    print(f"\n{col}:\n", df[col].value_counts())

fig, ax = plt.subplots(1, 1, figsize=(8, 5))
df[categorical_features[0]].value_counts().plot(kind='bar', ax=ax, color='lightgreen', edgecolor='black')
ax.set_title(categorical_features[0], fontweight='bold')
ax.set_ylabel('Frecuencia')
plt.tight_layout()
# plt.savefig('06_categoricas_distribucion.png', dpi=300)
# print("✓ Guardado: 06_categoricas_distribucion.png")
plt.show()

fig, ax = plt.subplots(1, 1, figsize=(8, 5))
df_clean.boxplot(column='toxicity_score', by=categorical_features[0], ax=ax)
ax.set_title(f'Toxicity por {categorical_features[0]}', fontweight='bold')
plt.suptitle('')
plt.tight_layout()
# plt.savefig('07_toxicity_por_categoria.png', dpi=300)
# print("✓ Guardado: 07_toxicity_por_categoria.png")
plt.show()

print("\nTOXICITY PROMEDIO POR CATEGORÍA:")
for col in categorical_features:
    print(f"\n{col}:\n", df_clean.groupby(col)['toxicity_score'].agg(['mean', 'median', 'std', 'count']))
```

## 1.6 Análisis de Texto

```{python}
#| label: analisis-texto

print("\nLONGITUD:\n", df['content_length'].describe())

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+|www.\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', '', text)
    return re.sub(r'\s+', ' ', text).strip()

# Usar stopwords de NLTK para español
try:
    stopwords_es = set(stopwords.words('spanish'))
except:
    import nltk
    nltk.download('stopwords')
    stopwords_es = set(stopwords.words('spanish'))

all_text = ' '.join(df['content'].apply(clean_text))
words = [w for w in all_text.split() if w and w not in stopwords_es and len(w) > 2]

print(f"\nTotal palabras (sin stopwords): {len(words)}")
print(f"Palabras únicas: {len(set(words))}")

word_freq = Counter(words)
top_words = word_freq.most_common(8)
print(f"\nTOP 8 PALABRAS:")
for word, count in top_words:
    print(f"  {word:20s}: {count:5d}")

fig, ax = plt.subplots(figsize=(10, 6))
words_df = pd.DataFrame(top_words, columns=['Palabra', 'Frecuencia'])
ax.barh(words_df['Palabra'], words_df['Frecuencia'], color='steelblue', edgecolor='black')
ax.set_xlabel('Frecuencia')
ax.set_title('Top 8 Palabras Más Frecuentes', fontweight='bold')
ax.invert_yaxis()
plt.tight_layout()
plt.show()

wordcloud = WordCloud(width=1600, height=800, background_color='white',
                     stopwords=stopwords_es, max_words=100, colormap='viridis').generate(' '.join(words))
fig, ax = plt.subplots(figsize=(16, 8))
ax.imshow(wordcloud, interpolation='bilinear')
ax.axis('off')
ax.set_title('Nube de Palabras', fontsize=16, fontweight='bold')
plt.tight_layout()
# plt.savefig('09_wordcloud.png', dpi=300)
# print("✓ Guardado: 09_wordcloud.png")
plt.show()
```

## 1.7 Hallazgos Clave del EDA

**Variables sin variabilidad:** Múltiples columnas fueron excluidas por contener valores constantes: `isReply` (100% True), `authorVerified` (100% False), `source` (100% "Twitter for iPhone"), y `hashtags_count` (100% cero). Estas variables no aportan información discriminativa para el análisis.

**Correlaciones débiles:** Todas las variables numéricas presentan correlaciones menores a 0.2 con `toxicity_score`, siendo `content_length` (0.196) la más alta. Esto indica que las features de metadata tienen poder predictivo limitado para detectar toxicidad.

**Distribución sesgada:** La mediana de toxicidad es 0.162, con la mayoría de tweets concentrados entre 0.0-0.3. Existen pocos tweets altamente tóxicos (>0.5), generando un dataset desbalanceado que dificulta la clasificación y limita el rendimiento de los modelos.

# 2. Preprocesamiento y Codificación

```{python}
#| label: preprocesamiento

# Eliminar registros con toxicity_score nulo
df_clean = df.dropna(subset=['toxicity_score']).copy()
print(f"Registros eliminados (toxicity_score nulo): {len(df) - len(df_clean)}")
print(f"Registros restantes: {len(df_clean)}")

# Seleccionar solo columnas útiles identificadas en EDA
columns_to_keep = ['authorFollowers', 'time_response', 'account_age_days', 
                   'mentions_count', 'content_length', 'has_profile_picture',
                   'content', 'createdAt', 'toxicity_score']
df_clean = df_clean[columns_to_keep]
print(f"Columnas seleccionadas: {len(columns_to_keep) - 1} features + 1 target")

# BLOQUE 2: FEATURE ENGINEERING
# Feature Engineering

# Extraer features de fecha
df_clean['createdAt'] = pd.to_datetime(df_clean['createdAt'])
df_clean['hour_of_day'] = df_clean['createdAt'].dt.hour
df_clean['day_of_week'] = df_clean['createdAt'].dt.dayofweek
print("✓ Features de fecha extraídas: hour_of_day, day_of_week")

# Transformación logarítmica de authorFollowers
df_clean['authorFollowers_log'] = np.log1p(df_clean['authorFollowers'])
print("✓ Transformación logarítmica aplicada a authorFollowers")

# Eliminar columnas originales no necesarias
df_clean = df_clean.drop(['createdAt', 'authorFollowers'], axis=1)

# Eliminación de outliers

def remove_outliers_iqr(df, columns):
    df_no_outliers = df.copy()
    outliers_removed = 0
    
    for col in columns:
        Q1 = df_no_outliers[col].quantile(0.25)
        Q3 = df_no_outliers[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        
        before = len(df_no_outliers)
        df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower) & (df_no_outliers[col] <= upper)]
        removed = before - len(df_no_outliers)
        outliers_removed += removed
        
        if removed > 0:
            print(f"  {col}: {removed} outliers eliminados")
    
    return df_no_outliers, outliers_removed

numeric_cols = ['authorFollowers_log', 'time_response', 'account_age_days', 
                'mentions_count', 'content_length']

df_clean, total_outliers = remove_outliers_iqr(df_clean, numeric_cols)
print(f"\nTotal outliers eliminados: {total_outliers}")
print(f"Registros finales: {len(df_clean)}")

# Construcción del pipeline

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Definir features por tipo
numeric_features = ['authorFollowers_log', 'time_response', 'account_age_days', 
                   'mentions_count', 'content_length', 'hour_of_day']
categorical_features = ['has_profile_picture', 'day_of_week']
text_feature = 'content'

print(f"Variables numéricas: {len(numeric_features)}")
print(f"Variables categóricas: {len(categorical_features)}")
print(f"Variable de texto: 1 (content)")

# Cargar stopwords de NLTK
try:
    stopwords_list = list(stopwords.words('spanish'))
except:
    import nltk
    nltk.download('stopwords')
    stopwords_list = list(stopwords.words('spanish'))


# TweetTokenizer para procesamiento específico de tweets
from nltk.tokenize import TweetTokenizer
tweet_tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)

# Transformadores
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)
text_transformer = CountVectorizer(max_features=500, ngram_range=(1,2), 
                                   min_df=2, stop_words=stopwords_list,
                                   tokenizer=tweet_tokenizer.tokenize)

# ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features),
        ('text', text_transformer, text_feature)
    ])

# Pipeline completo
pipeline = Pipeline([
    ('preprocessor', preprocessor)
])


# BLOQUE 5: APLICAR TRANSFORMACIÓN Y GUARDAR


# Separar features y target
X = df_clean[numeric_features + categorical_features + [text_feature]]
y = df_clean['toxicity_score']

print(f"Forma de X: {X.shape}")
print(f"Forma de y: {y.shape}")

# Aplicar transformación
X_transformed = pipeline.fit_transform(X)
print(f"\nForma de X transformada: {X_transformed.shape}")
print(f"  - Features numéricas: {len(numeric_features)}")
print(f"  - Features categóricas (one-hot): ~{len(categorical_features) * 2}")
print(f"  - Features de texto (CountVectorizer): 500")
print(f"  - Total: {X_transformed.shape[1]}")

# Convertir a array denso si es sparse
from scipy.sparse import issparse
if issparse(X_transformed):
    X_transformed = X_transformed.toarray()
```

# 3. Clasificación

## 3.1 Transformación a Variable Binaria

**Justificación del umbral:** Los scatterplots del EDA muestran que no existe separación natural entre clases en el espacio de features. La toxicidad es un continuo sin clusters evidentes. Se utiliza la mediana como umbral pragmático para dividir el dataset en dos grupos balanceados (Baja/Alta toxicidad), facilitando el entrenamiento supervisado y evitando desbalanceo de clases.

```{python}
#| label: target-binario

# Calcular mediana y crear variable binaria
threshold = y.median()
y_binary = (y >= threshold).astype(int)

print(f"Umbral seleccionado: {threshold:.3f} (mediana)")
print(f"Clase 0 (Baja toxicidad): {(y_binary == 0).sum()} ({(y_binary == 0).sum()/len(y_binary)*100:.1f}%)")
print(f"Clase 1 (Alta toxicidad): {(y_binary == 1).sum()} ({(y_binary == 1).sum()/len(y_binary)*100:.1f}%)")
```

## 3.2 Entrenamiento del Modelo

```{python}
#| label: entrenamiento-clasificacion

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.metrics import classification_report

# División train/test
X_train, X_test, y_train, y_test = train_test_split(
    X_transformed, y_binary, test_size=0.2, random_state=42, stratify=y_binary
)

print(f"Conjunto de entrenamiento: {X_train.shape[0]} registros")
print(f"Conjunto de prueba: {X_test.shape[0]} registros")

# Entrenar Random Forest
print("\nAlgoritmo seleccionado: Random Forest")
print("  Razón: Maneja bien features mixtas, robusto y proporciona feature importance")

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)
print("✓ Modelo entrenado")

# Evaluación del modelo

# Predicciones
y_pred = rf_model.predict(X_test)
y_pred_proba = rf_model.predict_proba(X_test)[:, 1]

# Métricas
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print("\nMétricas de rendimiento:")
print(f"  Accuracy:  {accuracy:.4f}")
print(f"  Precision: {precision:.4f}")
print(f"  Recall:    {recall:.4f}")
print(f"  F1-Score:  {f1:.4f}")
print(f"  ROC-AUC:   {roc_auc:.4f}")

print("\nReporte de clasificación detallado:")
print(classification_report(y_test, y_pred, target_names=['Baja toxicidad', 'Alta toxicidad']))

# Visualizaciones

# Matriz de confusión
cm = confusion_matrix(y_test, y_pred)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Subplot 1: Matriz de confusión
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], 
            xticklabels=['Baja', 'Alta'], yticklabels=['Baja', 'Alta'])
axes[0].set_xlabel('Predicción')
axes[0].set_ylabel('Real')
axes[0].set_title('Matriz de Confusión', fontweight='bold')

# Subplot 2: Curva ROC
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc_plot = auc(fpr, tpr)

axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc_plot:.4f})')
axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
axes[1].set_xlabel('False Positive Rate')
axes[1].set_ylabel('True Positive Rate')
axes[1].set_title('Curva ROC', fontweight='bold')
axes[1].legend(loc='lower right')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

# 4. Regresión

```{python}
#| label: regresion

# Usar el y continuo original (no binario)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_transformed, y, test_size=0.2, random_state=42
)

print(f"Conjunto de entrenamiento: {X_train_reg.shape[0]} registros")
print(f"Conjunto de prueba: {X_test_reg.shape[0]} registros")
print(f"Variable objetivo: toxicity_score (continua, rango 0-1)")

# Matriz de correlación

# Crear dataframe con features numéricas + target para correlación
df_corr = df_clean[numeric_features + ['toxicity_score']].copy()

correlation_matrix = df_corr.corr()

print("\nCorrelaciones con toxicity_score:")
print(correlation_matrix['toxicity_score'].sort_values(ascending=False))

# Visualizar matriz de correlación
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', 
            center=0, square=True, linewidths=1, ax=ax)
ax.set_title('Matriz de Correlación - Features Numéricas', fontweight='bold')
plt.tight_layout()
plt.show()

# Entrenamiento de modelos

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Modelo 1: Linear Regression
print("\nModelo 1: Linear Regression")
lr_model = LinearRegression()
lr_model.fit(X_train_reg, y_train_reg)
print("✓ Linear Regression entrenado")

# Modelo 2: Random Forest Regressor
print("\nModelo 2: Random Forest Regressor")
rfr_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rfr_model.fit(X_train_reg, y_train_reg)
print("✓ Random Forest Regressor entrenado")

# Evaluación de modelos

# Predicciones
y_pred_lr = lr_model.predict(X_test_reg)
y_pred_rfr = rfr_model.predict(X_test_reg)

# Métricas Linear Regression
mae_lr = mean_absolute_error(y_test_reg, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test_reg, y_pred_lr))
r2_lr = r2_score(y_test_reg, y_pred_lr)

# Métricas Random Forest
mae_rfr = mean_absolute_error(y_test_reg, y_pred_rfr)
rmse_rfr = np.sqrt(mean_squared_error(y_test_reg, y_pred_rfr))
r2_rfr = r2_score(y_test_reg, y_pred_rfr)

print("\nMétricas - Linear Regression:")
print(f"  MAE:  {mae_lr:.4f}")
print(f"  RMSE: {rmse_lr:.4f}")
print(f"  R²:   {r2_lr:.4f}")

print("\nMétricas - Random Forest Regressor:")
print(f"  MAE:  {mae_rfr:.4f}")
print(f"  RMSE: {rmse_rfr:.4f}")
print(f"  R²:   {r2_rfr:.4f}")

# Visualizaciones

# Figura 1: Linear Regression
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# LR: Real vs Predicho
axes[0].scatter(y_test_reg, y_pred_lr, alpha=0.5, s=30)
axes[0].plot([y_test_reg.min(), y_test_reg.max()], 
             [y_test_reg.min(), y_test_reg.max()], 
             'r--', lw=2, label='Predicción perfecta')
axes[0].set_xlabel('Toxicity Real')
axes[0].set_ylabel('Toxicity Predicha')
axes[0].set_title(f'Linear Regression - Real vs Predicho\nR² = {r2_lr:.4f}', fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# LR: Residuales
residuals_lr = y_test_reg - y_pred_lr
axes[1].scatter(y_pred_lr, residuals_lr, alpha=0.5, s=30)
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('Toxicity Predicha')
axes[1].set_ylabel('Residuales (Real - Predicho)')
axes[1].set_title('Linear Regression - Residuales', fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Figura 2: Random Forest Regressor
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# RFR: Real vs Predicho
axes[0].scatter(y_test_reg, y_pred_rfr, alpha=0.5, s=30, color='green')
axes[0].plot([y_test_reg.min(), y_test_reg.max()], 
             [y_test_reg.min(), y_test_reg.max()], 
             'r--', lw=2, label='Predicción perfecta')
axes[0].set_xlabel('Toxicity Real')
axes[0].set_ylabel('Toxicity Predicha')
axes[0].set_title(f'Random Forest - Real vs Predicho\nR² = {r2_rfr:.4f}', fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# RFR: Residuales
residuals_rfr = y_test_reg - y_pred_rfr
axes[1].scatter(y_pred_rfr, residuals_rfr, alpha=0.5, s=30, color='green')
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[1].set_xlabel('Toxicity Predicha')
axes[1].set_ylabel('Residuales (Real - Predicho)')
axes[1].set_title('Random Forest - Residuales', fontweight='bold')
axes[1].grid(alpha=0.3)

plt.tight_layout()
# plt.savefig('13_regresion_random_forest.png', dpi=300, bbox_inches='tight')
# print("✓ Guardado: 13_regresion_random_forest.png")
plt.show()
```

# 5. Clustering

```{python}
#| label: clustering

import os
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore', category=Warning)

# Método 1: Elbow (Codo) - Incluir K=1
print("\nMétodo 1: Elbow (Inercia)")
inertias = []
K_range = range(1, 11)

for k in K_range:
    if k == 1:
        # K=1: todos los puntos en un cluster, inercia = varianza total
        inertias.append(np.sum(np.var(X_transformed, axis=0)) * X_transformed.shape[0])
    else:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_transformed)
        inertias.append(kmeans.inertia_)

print(f"  Inercia K=1: {inertias[0]:.2f}")
print(f"  Inercia K=2: {inertias[1]:.2f}")
print(f"  Reducción K=1→K=2: {(inertias[0]-inertias[1])/inertias[0]*100:.1f}%")

# Método 2: Silhouette Score (solo K>=2)
print("\nMétodo 2: Silhouette Score")
silhouette_scores = []
K_range_sil = range(2, 11)

for k in K_range_sil:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_transformed)
    score = silhouette_score(X_transformed, labels)
    silhouette_scores.append(score)
    print(f"  K={k}: Silhouette = {score:.4f}")

# Visualizar ambos métodos
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Elbow
axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
axes[0].set_xlabel('Número de Clusters (K)')
axes[0].set_ylabel('Inercia')
axes[0].set_title('Método del Codo (Elbow)', fontweight='bold')
axes[0].grid(alpha=0.3)
axes[0].set_xticks(K_range)

# Silhouette
axes[1].plot(K_range_sil, silhouette_scores, 'go-', linewidth=2, markersize=8)
axes[1].set_xlabel('Número de Clusters (K)')
axes[1].set_ylabel('Silhouette Score')
axes[1].set_title('Silhouette Score (mayor es mejor)', fontweight='bold')
axes[1].grid(alpha=0.3)
axes[1].set_xticks(K_range_sil)

plt.tight_layout()
plt.show()

# Seleccionar K óptimo basado en Silhouette
K_optimo = K_range_sil[np.argmax(silhouette_scores)]
print(f"\nK óptimo (máximo Silhouette): {K_optimo}")

# Entrenamiento de modelos de clustering

# K-Means
print(f"\nModelo 1: K-Means (K={K_optimo})")
kmeans_model = KMeans(n_clusters=K_optimo, random_state=42, n_init=10)
clusters_kmeans = kmeans_model.fit_predict(X_transformed)
print(f"✓ K-Means entrenado")
print(f"  Distribución de clusters: {np.bincount(clusters_kmeans)}")

# Agglomerative Clustering
print(f"\nModelo 2: Agglomerative Clustering (K={K_optimo})")
agg_model = AgglomerativeClustering(n_clusters=K_optimo)
clusters_agg = agg_model.fit_predict(X_transformed)
print(f"✓ Agglomerative Clustering entrenado")
print(f"  Distribución de clusters: {np.bincount(clusters_agg)}")

# Visualización de clusters

# Reducir dimensionalidad con PCA para visualizar
print("Aplicando PCA para reducir a 2 dimensiones...")
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_transformed)
print(f"Varianza explicada por PCA: {pca.explained_variance_ratio_.sum():.2%}")

# Visualización
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# K-Means
scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_kmeans, 
                           cmap='viridis', alpha=0.6, s=30)
axes[0].set_xlabel('Componente Principal 1')
axes[0].set_ylabel('Componente Principal 2')
axes[0].set_title(f'K-Means Clustering (K={K_optimo})', fontweight='bold')
plt.colorbar(scatter1, ax=axes[0], label='Cluster')

# Agglomerative
scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters_agg, 
                           cmap='viridis', alpha=0.6, s=30)
axes[1].set_xlabel('Componente Principal 1')
axes[1].set_ylabel('Componente Principal 2')
axes[1].set_title(f'Agglomerative Clustering (K={K_optimo})', fontweight='bold')
plt.colorbar(scatter2, ax=axes[1], label='Cluster')

plt.tight_layout()
# plt.savefig('15_clusters_visualizacion.png', dpi=300, bbox_inches='tight')
# print("✓ Guardado: 15_clusters_visualizacion.png")
plt.show()

# Análisis de clusters

# Calcular Silhouette Score para cada modelo
sil_kmeans = silhouette_score(X_transformed, clusters_kmeans)
sil_agg = silhouette_score(X_transformed, clusters_agg)

print("\nCalidad de los clusters (Silhouette Score):")
print(f"  K-Means: {sil_kmeans:.4f}")
print(f"  Agglomerative: {sil_agg:.4f}")
print(f"\n  Mejor modelo: {'K-Means' if sil_kmeans > sil_agg else 'Agglomerative'}")

print("\nInterpretación Silhouette Score:")
print("  > 0.5: Clusters bien definidos")
print("  0.25-0.5: Clusters con solapamiento moderado")
print("  < 0.25: Clusters mal definidos")

## 5.4 Reflexión: Clustering vs Clasificación Supervisada

#| label: comparacion-supervisado

from sklearn.metrics import adjusted_rand_score

ari_kmeans = adjusted_rand_score(y_binary, clusters_kmeans)
ari_agg = adjusted_rand_score(y_binary, clusters_agg)

print(f"Adjusted Rand Index (similitud con clases de toxicidad):")
print(f"  K-Means: {ari_kmeans:.4f}")
print(f"  Agglomerative: {ari_agg:.4f}")
```

**Comparación de enfoques:** La clasificación supervisada (Random Forest, ROC-AUC=0.75) utiliza etiquetas de toxicidad para aprender patrones y logra rendimiento moderado. En contraste, el clustering no supervisado (K-Means/Agglomerative, Silhouette~0.10) busca grupos naturales sin etiquetas, resultando en clusters mal definidos con mucho solapamiento. El Adjusted Rand Index cercano a 0 confirma que los clusters NO replican las clases de toxicidad, ya que agrupan tweets por similitud de contenido/features donde la toxicidad no es el factor discriminante principal. Tweets tóxicos y no tóxicos comparten vocabulario y características, demostrando que la toxicidad no emerge naturalmente de los datos y requiere aprendizaje supervisado para su detección efectiva.

# 6. Conclusiones Finales

## 6.1 Análisis de Resultados

El análisis reveló limitaciones críticas en los datos: las features numéricas tienen correlaciones extremadamente débiles con toxicidad (<0.2), múltiples columnas booleanas contenían valores constantes sin variabilidad útil, y la distribución de toxicidad está sesgada hacia valores bajos (mediana 0.162, mayoría entre 0.0-0.3) con pocos tweets realmente tóxicos. La clasificación supervisada alcanzó ROC-AUC de 0.75 (moderado, mejoró con TweetTokenizer), mientras que la regresión lineal falló completamente (R² negativo) y Random Forest solo explicó 32% de la varianza. En clustering, tanto el análisis visual previo como los métodos estadísticos (Elbow y Silhouette) coincidieron en K=2 como óptimo, pero el Silhouette Score de 0.10 confirma que no existen agrupaciones naturales bien definidas. Crucialmente, dividir en 2 clusters no implica que uno represente toxicidad y otro no; dado que la mayoría de tweets tienen toxicidad 0.0-0.3, ambos clusters contienen tweets predominantemente no tóxicos, solo con diferencias de grado, no de clase.

## 6.2 Limitaciones Principales

El dataset de 1500 tweets es pequeño para entrenar modelos robustos. CountVectorizer con TweetTokenizer, aunque mejoró el rendimiento, sigue siendo una representación bag-of-words que ignora contexto semántico y orden de palabras. Las features de metadata no aportan valor predictivo, y columnas que parecían prometedoras (booleanas, identificadores) resultaron inútiles por falta de variabilidad o alta cardinalidad sin información discriminativa.

## 6.3 Mejoras Propuestas

Utilizar embeddings por tweet (Word2Vec, GloVe, o BERT) para capturar significado contextual en lugar de conteo de palabras. Aumentar el tamaño del dataset (10k+ tweets) con mejor balance entre clases tóxicas y no tóxicas. Incorporar features de ingeniería como análisis de sentimiento, ratio de mayúsculas, uso de signos de exclamación, y métricas de engagement. Considerar modelos de deep learning (LSTM, Transformers) que pueden aprender representaciones más complejas del lenguaje tóxico. Explorar técnicas de data augmentation para balancear clases.

