[
  {
    "objectID": "RESUMEN_MODELOS.html",
    "href": "RESUMEN_MODELOS.html",
    "title": "Resumen de Modelos de Machine Learning - Aprendizaje Supervisado",
    "section": "",
    "text": "Este documento presenta los resultados de dos problemas independientes de aprendizaje supervisado utilizando datos abiertos de Ecuador: un modelo de regresi√≥n para predecir nivel socioecon√≥mico y un modelo de clasificaci√≥n binaria para predecir siniestros de tr√°nsito letales.\n\n\n\n\n\n\nPredecir el decil socioecon√≥mico (1-10) de hogares ecuatorianos basado en caracter√≠sticas de vivienda y acceso a servicios b√°sicos.\n\n\n\n\nFuente: Datos Abiertos Ecuador - Encuesta de Hogares 2018\nTama√±o: 2,565,433 registros de hogares\nVariables: 17 features (materiales de vivienda, servicios b√°sicos, ubicaci√≥n geogr√°fica)\nVariable objetivo: Decil socioecon√≥mico (1 = m√°s pobre, 10 = m√°s rico)\n\n\n\n\n\nModelo: LinearRegression con Pipeline\nPreprocesamiento: StandardScaler + LabelEncoder\nDivisi√≥n: 80% entrenamiento, 20% prueba (estratificada)\nFeatures principales: Tipo de vivienda, materiales (techo/piso/paredes), servicios (agua, electricidad, internet, TV cable)\n\n\n\n\n\n\n\nM√©trica\nValor\nInterpretaci√≥n\n\n\n\n\nR¬≤\n0.38\nExplica 38% de la varianza (capacidad moderada)\n\n\nMSE\n3.46\nError cuadr√°tico medio aceptable\n\n\nRMSE\n1.86\nError promedio de ~2 deciles\n\n\n\n\n\n\n\nTotal de personas por hogar (coef: -1.96) ‚Üí Hogares m√°s grandes tienden a menor decil\nTotal de hogares por √°rea (coef: +1.86) ‚Üí M√°s hogares concentrados = mayor decil\nMaterial del piso (coef: -0.50) ‚Üí Mejores materiales = mayor decil\nAcceso a TV cable/satelital (coef: -0.39) ‚Üí Acceso mejora el decil\nUbicaci√≥n urbana/rural (coef: -0.29) ‚Üí √Årea urbana = mayor decil\n\n\n\n\n\nEl modelo identifica patrones claros entre infraestructura de vivienda y nivel socioecon√≥mico\nServicios b√°sicos (internet, TV, tel√©fono) son predictores importantes del bienestar\n√Årea geogr√°fica influye significativamente en el nivel socioecon√≥mico\nAplicaciones: Identificaci√≥n de hogares vulnerables, planificaci√≥n de programas sociales, inversi√≥n en infraestructura\n\n\n\n\n\n\n\n\nPredecir si un siniestro de tr√°nsito ser√° letal (con fallecidos) o no letal basado en caracter√≠sticas del accidente.\n\n\n\n\nFuente: Datos Abiertos Ecuador - INEC Siniestros de Tr√°nsito 2019\nTama√±o: 24,595 siniestros de tr√°nsito\nVariable objetivo: Binaria (Letal vs No Letal)\nDistribuci√≥n: 8.01% letales, 91.99% no letales (clases desbalanceadas)\n\n\n\n\n\nModelo: LogisticRegression con Pipeline\nPreprocesamiento: OneHotEncoder + StandardScaler\nDivisi√≥n: 80% entrenamiento, 20% prueba (estratificada)\nFeatures principales: Ubicaci√≥n (provincia, cant√≥n), tipo de siniestro, causa, hora, zona urbana/rural\n\n\n\n\n\n\n\n\n\n\n\n\nM√©trica\nValor\nInterpretaci√≥n\n\n\n\n\nAccuracy\n92.4%\nClasifica correctamente 9 de cada 10 casos\n\n\nPrecision\n66.1%\nDe los predichos como letales, 66% son realmente letales\n\n\nRecall\n9.9%\nSolo detecta 1 de cada 10 siniestros letales reales\n\n\nF1-Score\n0.17\nBalance bajo entre precision y recall\n\n\nROC-AUC\n0.81\nExcelente capacidad de discriminaci√≥n\n\n\n\n\n\n\n\nCant√≥n El Carmen ‚Üí Mayor riesgo de siniestros letales\nProvincia Cotopaxi ‚Üí Zona de alto riesgo\nClase ‚ÄúCa√≠da de Pasajeros‚Äù ‚Üí Menor probabilidad de ser letal (coef negativo)\nCant√≥n Puerto Quito ‚Üí Alto riesgo de letalidad\nZona urbana ‚Üí Menor riesgo que zonas rurales\n\n\n\n\n\nEl modelo es conservador: prefiere no clasificar como letal (bajo recall)\nExcelente accuracy general pero baja detecci√≥n de casos letales\nFactores geogr√°ficos son los m√°s predictivos de letalidad\nProblema de clases desbalanceadas afecta el rendimiento\nAplicaciones: Prevenci√≥n de accidentes, asignaci√≥n de recursos de emergencia, pol√≠ticas de seguridad vial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspecto\nRegresi√≥n\nClasificaci√≥n\n\n\n\n\nTipo de problema\nPredicci√≥n continua\nPredicci√≥n binaria\n\n\nRendimiento\nModerado (R¬≤=0.38)\nAlto (Acc=92.4%)\n\n\nPrincipal desaf√≠o\nVarianza no explicada\nClases desbalanceadas\n\n\nFactor m√°s importante\nTotal personas/hogares\nUbicaci√≥n geogr√°fica\n\n\nAplicaci√≥n pr√°ctica\nPol√≠ticas sociales\nSeguridad vial\n\n\n\n\n\n\n\n\n# Regresi√≥n\nPipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', LinearRegression())\n])\n\n# Clasificaci√≥n\nPipeline([\n    ('preprocessor', ColumnTransformer([\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])),\n    ('classifier', LogisticRegression())\n])\n\n\n\n\nRegresi√≥n: Valores reales vs predichos, curva de aprendizaje, distribuci√≥n de errores\nClasificaci√≥n: Matriz de confusi√≥n, curva ROC, distribuci√≥n de probabilidades, m√©tricas de evaluaci√≥n\n\n\n\n\n\n\nAmbos modelos demuestran la aplicabilidad del aprendizaje supervisado a problemas reales ecuatorianos\nFactores geogr√°ficos emergen como predictores importantes en ambos casos\nPipeline de Scikit-learn facilita el preprocesamiento y entrenamiento sistem√°tico\nDatos gubernamentales abiertos proporcionan oportunidades valiosas para an√°lisis predictivo\nConsideraciones √©ticas son importantes, especialmente en aplicaciones de pol√≠tica p√∫blica\n\n\n\n\n\n\n\n\nregresion_hogares.py - C√≥digo principal del modelo\ndecil_reales_vs_predichos.png - Comparaci√≥n de predicciones\ncurva_aprendizaje_decil.png - Curva de aprendizaje\nimportancia_factores_decil.png - Importancia de features\n\n\n\n\n\nclasificacion_binaria.py - C√≥digo principal del modelo\nmatriz_confusion.png - Matriz de confusi√≥n\ncurva_roc.png - Curva ROC\ndistribucion_probabilidades.png - Distribuci√≥n de probabilidades\nmetricas_evaluacion.png - M√©tricas de evaluaci√≥n\n\n\nProyecto desarrollado como parte del curso de Machine Learning - Aplicaci√≥n de t√©cnicas de aprendizaje supervisado con datos abiertos de Ecuador."
  },
  {
    "objectID": "RESUMEN_MODELOS.html#descripci√≥n-general",
    "href": "RESUMEN_MODELOS.html#descripci√≥n-general",
    "title": "Resumen de Modelos de Machine Learning - Aprendizaje Supervisado",
    "section": "",
    "text": "Este documento presenta los resultados de dos problemas independientes de aprendizaje supervisado utilizando datos abiertos de Ecuador: un modelo de regresi√≥n para predecir nivel socioecon√≥mico y un modelo de clasificaci√≥n binaria para predecir siniestros de tr√°nsito letales."
  },
  {
    "objectID": "RESUMEN_MODELOS.html#modelo-de-regresi√≥n---predicci√≥n-de-decil-socioecon√≥mico",
    "href": "RESUMEN_MODELOS.html#modelo-de-regresi√≥n---predicci√≥n-de-decil-socioecon√≥mico",
    "title": "Resumen de Modelos de Machine Learning - Aprendizaje Supervisado",
    "section": "",
    "text": "Predecir el decil socioecon√≥mico (1-10) de hogares ecuatorianos basado en caracter√≠sticas de vivienda y acceso a servicios b√°sicos.\n\n\n\n\nFuente: Datos Abiertos Ecuador - Encuesta de Hogares 2018\nTama√±o: 2,565,433 registros de hogares\nVariables: 17 features (materiales de vivienda, servicios b√°sicos, ubicaci√≥n geogr√°fica)\nVariable objetivo: Decil socioecon√≥mico (1 = m√°s pobre, 10 = m√°s rico)\n\n\n\n\n\nModelo: LinearRegression con Pipeline\nPreprocesamiento: StandardScaler + LabelEncoder\nDivisi√≥n: 80% entrenamiento, 20% prueba (estratificada)\nFeatures principales: Tipo de vivienda, materiales (techo/piso/paredes), servicios (agua, electricidad, internet, TV cable)\n\n\n\n\n\n\n\nM√©trica\nValor\nInterpretaci√≥n\n\n\n\n\nR¬≤\n0.38\nExplica 38% de la varianza (capacidad moderada)\n\n\nMSE\n3.46\nError cuadr√°tico medio aceptable\n\n\nRMSE\n1.86\nError promedio de ~2 deciles\n\n\n\n\n\n\n\nTotal de personas por hogar (coef: -1.96) ‚Üí Hogares m√°s grandes tienden a menor decil\nTotal de hogares por √°rea (coef: +1.86) ‚Üí M√°s hogares concentrados = mayor decil\nMaterial del piso (coef: -0.50) ‚Üí Mejores materiales = mayor decil\nAcceso a TV cable/satelital (coef: -0.39) ‚Üí Acceso mejora el decil\nUbicaci√≥n urbana/rural (coef: -0.29) ‚Üí √Årea urbana = mayor decil\n\n\n\n\n\nEl modelo identifica patrones claros entre infraestructura de vivienda y nivel socioecon√≥mico\nServicios b√°sicos (internet, TV, tel√©fono) son predictores importantes del bienestar\n√Årea geogr√°fica influye significativamente en el nivel socioecon√≥mico\nAplicaciones: Identificaci√≥n de hogares vulnerables, planificaci√≥n de programas sociales, inversi√≥n en infraestructura"
  },
  {
    "objectID": "RESUMEN_MODELOS.html#modelo-de-clasificaci√≥n-binaria---predicci√≥n-de-siniestros-letales",
    "href": "RESUMEN_MODELOS.html#modelo-de-clasificaci√≥n-binaria---predicci√≥n-de-siniestros-letales",
    "title": "Resumen de Modelos de Machine Learning - Aprendizaje Supervisado",
    "section": "",
    "text": "Predecir si un siniestro de tr√°nsito ser√° letal (con fallecidos) o no letal basado en caracter√≠sticas del accidente.\n\n\n\n\nFuente: Datos Abiertos Ecuador - INEC Siniestros de Tr√°nsito 2019\nTama√±o: 24,595 siniestros de tr√°nsito\nVariable objetivo: Binaria (Letal vs No Letal)\nDistribuci√≥n: 8.01% letales, 91.99% no letales (clases desbalanceadas)\n\n\n\n\n\nModelo: LogisticRegression con Pipeline\nPreprocesamiento: OneHotEncoder + StandardScaler\nDivisi√≥n: 80% entrenamiento, 20% prueba (estratificada)\nFeatures principales: Ubicaci√≥n (provincia, cant√≥n), tipo de siniestro, causa, hora, zona urbana/rural\n\n\n\n\n\n\n\n\n\n\n\n\nM√©trica\nValor\nInterpretaci√≥n\n\n\n\n\nAccuracy\n92.4%\nClasifica correctamente 9 de cada 10 casos\n\n\nPrecision\n66.1%\nDe los predichos como letales, 66% son realmente letales\n\n\nRecall\n9.9%\nSolo detecta 1 de cada 10 siniestros letales reales\n\n\nF1-Score\n0.17\nBalance bajo entre precision y recall\n\n\nROC-AUC\n0.81\nExcelente capacidad de discriminaci√≥n\n\n\n\n\n\n\n\nCant√≥n El Carmen ‚Üí Mayor riesgo de siniestros letales\nProvincia Cotopaxi ‚Üí Zona de alto riesgo\nClase ‚ÄúCa√≠da de Pasajeros‚Äù ‚Üí Menor probabilidad de ser letal (coef negativo)\nCant√≥n Puerto Quito ‚Üí Alto riesgo de letalidad\nZona urbana ‚Üí Menor riesgo que zonas rurales\n\n\n\n\n\nEl modelo es conservador: prefiere no clasificar como letal (bajo recall)\nExcelente accuracy general pero baja detecci√≥n de casos letales\nFactores geogr√°ficos son los m√°s predictivos de letalidad\nProblema de clases desbalanceadas afecta el rendimiento\nAplicaciones: Prevenci√≥n de accidentes, asignaci√≥n de recursos de emergencia, pol√≠ticas de seguridad vial"
  },
  {
    "objectID": "RESUMEN_MODELOS.html#comparaci√≥n-de-modelos",
    "href": "RESUMEN_MODELOS.html#comparaci√≥n-de-modelos",
    "title": "Resumen de Modelos de Machine Learning - Aprendizaje Supervisado",
    "section": "",
    "text": "Aspecto\nRegresi√≥n\nClasificaci√≥n\n\n\n\n\nTipo de problema\nPredicci√≥n continua\nPredicci√≥n binaria\n\n\nRendimiento\nModerado (R¬≤=0.38)\nAlto (Acc=92.4%)\n\n\nPrincipal desaf√≠o\nVarianza no explicada\nClases desbalanceadas\n\n\nFactor m√°s importante\nTotal personas/hogares\nUbicaci√≥n geogr√°fica\n\n\nAplicaci√≥n pr√°ctica\nPol√≠ticas sociales\nSeguridad vial"
  },
  {
    "objectID": "RESUMEN_MODELOS.html#metodolog√≠a-t√©cnica",
    "href": "RESUMEN_MODELOS.html#metodolog√≠a-t√©cnica",
    "title": "Resumen de Modelos de Machine Learning - Aprendizaje Supervisado",
    "section": "",
    "text": "# Regresi√≥n\nPipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', LinearRegression())\n])\n\n# Clasificaci√≥n\nPipeline([\n    ('preprocessor', ColumnTransformer([\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])),\n    ('classifier', LogisticRegression())\n])\n\n\n\n\nRegresi√≥n: Valores reales vs predichos, curva de aprendizaje, distribuci√≥n de errores\nClasificaci√≥n: Matriz de confusi√≥n, curva ROC, distribuci√≥n de probabilidades, m√©tricas de evaluaci√≥n"
  },
  {
    "objectID": "RESUMEN_MODELOS.html#conclusiones-generales",
    "href": "RESUMEN_MODELOS.html#conclusiones-generales",
    "title": "Resumen de Modelos de Machine Learning - Aprendizaje Supervisado",
    "section": "",
    "text": "Ambos modelos demuestran la aplicabilidad del aprendizaje supervisado a problemas reales ecuatorianos\nFactores geogr√°ficos emergen como predictores importantes en ambos casos\nPipeline de Scikit-learn facilita el preprocesamiento y entrenamiento sistem√°tico\nDatos gubernamentales abiertos proporcionan oportunidades valiosas para an√°lisis predictivo\nConsideraciones √©ticas son importantes, especialmente en aplicaciones de pol√≠tica p√∫blica"
  },
  {
    "objectID": "RESUMEN_MODELOS.html#archivos-del-proyecto",
    "href": "RESUMEN_MODELOS.html#archivos-del-proyecto",
    "title": "Resumen de Modelos de Machine Learning - Aprendizaje Supervisado",
    "section": "",
    "text": "regresion_hogares.py - C√≥digo principal del modelo\ndecil_reales_vs_predichos.png - Comparaci√≥n de predicciones\ncurva_aprendizaje_decil.png - Curva de aprendizaje\nimportancia_factores_decil.png - Importancia de features\n\n\n\n\n\nclasificacion_binaria.py - C√≥digo principal del modelo\nmatriz_confusion.png - Matriz de confusi√≥n\ncurva_roc.png - Curva ROC\ndistribucion_probabilidades.png - Distribuci√≥n de probabilidades\nmetricas_evaluacion.png - M√©tricas de evaluaci√≥n\n\n\nProyecto desarrollado como parte del curso de Machine Learning - Aplicaci√≥n de t√©cnicas de aprendizaje supervisado con datos abiertos de Ecuador."
  },
  {
    "objectID": "indexx.html",
    "href": "indexx.html",
    "title": "Primer proyecto en Quarto",
    "section": "",
    "text": "Este documento presenta un an√°lisis completo de machine learning utilizando Quarto para la documentaci√≥n y visualizaci√≥n de resultados.\n\n\n\nExplorar y analizar un conjunto de datos\nImplementar diferentes algoritmos de machine learning\nComparar el rendimiento de los modelos\nVisualizar los resultados de manera clara"
  },
  {
    "objectID": "indexx.html#objetivos",
    "href": "indexx.html#objetivos",
    "title": "Primer proyecto en Quarto",
    "section": "",
    "text": "Explorar y analizar un conjunto de datos\nImplementar diferentes algoritmos de machine learning\nComparar el rendimiento de los modelos\nVisualizar los resultados de manera clara"
  },
  {
    "objectID": "indexx.html#estad√≠sticas-descriptivas",
    "href": "indexx.html#estad√≠sticas-descriptivas",
    "title": "Primer proyecto en Quarto",
    "section": "Estad√≠sticas Descriptivas",
    "text": "Estad√≠sticas Descriptivas\n\n\nCode\n# Estad√≠sticas descriptivas\nprint(\"Estad√≠sticas descriptivas:\")\ndf.describe()\n\n\nEstad√≠sticas descriptivas:\n\n\n\n\n\n\n\n\n\nfeature_1\nfeature_2\nfeature_3\ntarget\n\n\n\n\ncount\n1000.000000\n1000.000000\n1000.000000\n1000.000000\n\n\nmean\n0.019332\n0.070836\n0.005834\n0.379000\n\n\nstd\n0.979216\n0.997454\n0.983454\n0.485381\n\n\nmin\n-3.241267\n-2.940389\n-3.019512\n0.000000\n\n\n25%\n-0.647590\n-0.606242\n-0.648000\n0.000000\n\n\n50%\n0.025301\n0.063077\n-0.000251\n0.000000\n\n\n75%\n0.647944\n0.728882\n0.660915\n1.000000\n\n\nmax\n3.852731\n3.193108\n3.926238\n1.000000"
  },
  {
    "objectID": "indexx.html#visualizaci√≥n-de-los-datos",
    "href": "indexx.html#visualizaci√≥n-de-los-datos",
    "title": "Primer proyecto en Quarto",
    "section": "Visualizaci√≥n de los Datos",
    "text": "Visualizaci√≥n de los Datos\n\n\nCode\n# Crear subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Histograma de feature_1\naxes[0, 0].hist(df['feature_1'], bins=30, alpha=0.7, color='skyblue')\naxes[0, 0].set_title('Distribuci√≥n de Feature 1')\naxes[0, 0].set_xlabel('Valor')\naxes[0, 0].set_ylabel('Frecuencia')\n\n# Scatter plot feature_1 vs feature_2\nscatter = axes[0, 1].scatter(df['feature_1'], df['feature_2'], \n                           c=df['target'], cmap='viridis', alpha=0.6)\naxes[0, 1].set_title('Feature 1 vs Feature 2')\naxes[0, 1].set_xlabel('Feature 1')\naxes[0, 1].set_ylabel('Feature 2')\nplt.colorbar(scatter, ax=axes[0, 1])\n\n# Box plot por target\ndf.boxplot(column='feature_1', by='target', ax=axes[1, 0])\naxes[1, 0].set_title('Feature 1 por Target')\naxes[1, 0].set_xlabel('Target')\n\n# Correlaci√≥n\ncorr_matrix = df.corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\naxes[1, 1].set_title('Matriz de Correlaci√≥n')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "indexx.html#modelo-1-regresi√≥n-log√≠stica",
    "href": "indexx.html#modelo-1-regresi√≥n-log√≠stica",
    "title": "Primer proyecto en Quarto",
    "section": "Modelo 1: Regresi√≥n Log√≠stica",
    "text": "Modelo 1: Regresi√≥n Log√≠stica\n\n\nCode\n# Entrenar modelo de regresi√≥n log√≠stica\nlr_model = LogisticRegression(random_state=42, max_iter=1000)\nlr_model.fit(X_train, y_train)\n\n# Predicciones\nlr_pred = lr_model.predict(X_test)\nlr_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n\nprint(\"Regresi√≥n Log√≠stica - M√©tricas:\")\nprint(classification_report(y_test, lr_pred))\n\n\nRegresi√≥n Log√≠stica - M√©tricas:\n              precision    recall  f1-score   support\n\n           0       0.62      1.00      0.77       124\n           1       0.00      0.00      0.00        76\n\n    accuracy                           0.62       200\n   macro avg       0.31      0.50      0.38       200\nweighted avg       0.38      0.62      0.47       200"
  },
  {
    "objectID": "indexx.html#modelo-2-random-forest",
    "href": "indexx.html#modelo-2-random-forest",
    "title": "Primer proyecto en Quarto",
    "section": "Modelo 2: Random Forest",
    "text": "Modelo 2: Random Forest\n\n\nCode\n# Entrenar modelo Random Forest\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predicciones\nrf_pred = rf_model.predict(X_test)\nrf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\nprint(\"Random Forest - M√©tricas:\")\nprint(classification_report(y_test, rf_pred))\n\n\nRandom Forest - M√©tricas:\n              precision    recall  f1-score   support\n\n           0       0.58      0.72      0.64       124\n           1       0.24      0.14      0.18        76\n\n    accuracy                           0.50       200\n   macro avg       0.41      0.43      0.41       200\nweighted avg       0.45      0.50      0.47       200"
  },
  {
    "objectID": "indexx.html#matriz-de-confusi√≥n",
    "href": "indexx.html#matriz-de-confusi√≥n",
    "title": "Primer proyecto en Quarto",
    "section": "Matriz de Confusi√≥n",
    "text": "Matriz de Confusi√≥n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Matriz de confusi√≥n para Regresi√≥n Log√≠stica\ncm_lr = confusion_matrix(y_test, lr_pred)\nsns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0])\naxes[0].set_title('Regresi√≥n Log√≠stica')\naxes[0].set_xlabel('Predicci√≥n')\naxes[0].set_ylabel('Valor Real')\n\n# Matriz de confusi√≥n para Random Forest\ncm_rf = confusion_matrix(y_test, rf_pred)\nsns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1])\naxes[1].set_title('Random Forest')\naxes[1].set_xlabel('Predicci√≥n')\naxes[1].set_ylabel('Valor Real')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "indexx.html#comparaci√≥n-de-importancia-de-caracter√≠sticas",
    "href": "indexx.html#comparaci√≥n-de-importancia-de-caracter√≠sticas",
    "title": "Primer proyecto en Quarto",
    "section": "Comparaci√≥n de Importancia de Caracter√≠sticas",
    "text": "Comparaci√≥n de Importancia de Caracter√≠sticas\n\n\nCode\n# Importancia de caracter√≠sticas para Random Forest\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=True)\n\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.title('Importancia de Caracter√≠sticas - Random Forest')\nplt.xlabel('Importancia')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "indexx.html#resumen-de-resultados",
    "href": "indexx.html#resumen-de-resultados",
    "title": "Primer proyecto en Quarto",
    "section": "Resumen de Resultados",
    "text": "Resumen de Resultados\nLos modelos implementados muestran diferentes niveles de rendimiento:\n\nRegresi√≥n Log√≠stica: Modelo lineal simple y r√°pido\nRandom Forest: Modelo m√°s complejo con mejor capacidad de capturar relaciones no lineales"
  },
  {
    "objectID": "indexx.html#pr√≥ximos-pasos",
    "href": "indexx.html#pr√≥ximos-pasos",
    "title": "Primer proyecto en Quarto",
    "section": "Pr√≥ximos Pasos",
    "text": "Pr√≥ximos Pasos\n\nProbar otros algoritmos (SVM, XGBoost, etc.)\nOptimizar hiperpar√°metros\nImplementar validaci√≥n cruzada\nAn√°lisis de m√°s caracter√≠sticas\n\n\nDocumento generado con Quarto - {format(Sys.time(), ‚Äú%Y-%m-%d %H:%M:%S‚Äù)}"
  },
  {
    "objectID": "clase_2-10-2025/clasificaion.html",
    "href": "clase_2-10-2025/clasificaion.html",
    "title": "Clasificaci√≥n",
    "section": "",
    "text": "Code\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n\n\n\nCargar el dataset\n\n\nCode\ndata = load_breast_cancer()\ndata\n\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n\n\n#entrenar modelo\n\n\nCode\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n\n/home/juanchx/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000)\n\n\n#predecir\n\n\nCode\ny_pred = model.predict(X_test)\n\n\n#metricas\n\n\nCode\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is one of the fundamental steps in any data science process. It allows us to understand the structure, detect anomalies, and uncover patterns in the data before modeling.\n\n‚ÄúWithout EDA, you‚Äôre not doing data science, you‚Äôre just guessing.‚Äù\n\nEDA combines statistics, programming, and visualization to explore datasets. This report is designed to help you practice these core skills using real-world data.\n\n\nWe will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet‚Äôs load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let‚Äôs examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset.\n\n\n\nBefore diving deeper into the data, it‚Äôs useful to explore some key metadata:\n\n‚úÖ The column names and their data types\n‚ö†Ô∏è The presence of missing values\nüìä Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we‚Äôre dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB\n\n\n\n\n\n\nDetecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet‚Äôs start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows √ó 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let‚Äôs set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis.\n\n\n\n\nFinally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows √ó 15 columns"
  },
  {
    "objectID": "eda.html#dataset",
    "href": "eda.html#dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "We will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet‚Äôs load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let‚Äôs examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset."
  },
  {
    "objectID": "eda.html#first-steps",
    "href": "eda.html#first-steps",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Before diving deeper into the data, it‚Äôs useful to explore some key metadata:\n\n‚úÖ The column names and their data types\n‚ö†Ô∏è The presence of missing values\nüìä Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we‚Äôre dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB"
  },
  {
    "objectID": "eda.html#missing-values",
    "href": "eda.html#missing-values",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Detecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet‚Äôs start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows √ó 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let‚Äôs set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis."
  },
  {
    "objectID": "eda.html#cleaned-dataset",
    "href": "eda.html#cleaned-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Finally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows √ó 15 columns"
  },
  {
    "objectID": "eda.html#univariate-analysis-quantitative",
    "href": "eda.html#univariate-analysis-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.1 Univariate Analysis: Quantitative",
    "text": "2.1 Univariate Analysis: Quantitative\nA univariate analysis focuses on examining a single numeric variable to understand its distribution, shape, central tendency, and spread. One of the most common tools for this is the histogram.\nIn this case, we‚Äôll explore the distribution of the movie runtime (Running_Time_min).\n\n2.1.1 Basic Histogram\nWe start by creating a histogram to visualize the distribution of running times:\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    title='Histogram of Movie Runtimes (30 bins)'\n)\n\n\n\n\n\n\n\nThis chart shows how many movies fall into each time interval (bin). However, histograms can look quite different depending on the number and size of bins used.\n\n\n2.1.2 Effect of Bin Size\nLet‚Äôs compare how the histogram shape changes with different bin sizes:\n\n\nCode\nhistogram_1 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=8)),\n    alt.Y('count()')\n)\n\nhistogram_2 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=10)),\n    alt.Y('count()')\n)\n\nhistogram_1 | histogram_2\n\n\n\n\n\n\n\nEven though both plots use the same data, the choice of bin size changes the visual interpretation. A small number of bins may hide details, while too many bins can make it harder to spot trends.\n\n\n2.1.3 Density plots, or Kernel Density Estimate (KDE)\nDensity plots offer a smoothed alternative to histograms. Instead of using rectangular bins to count data points, they estimate the probability density function by placing bell-shaped curves (kernels) at each observation and summing them.\nThis approach helps reduce the visual noise and jaggedness that can occur in histograms and gives a clearer picture of the underlying distribution.\n\n\nCode\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    as_=['Running_Time_min','density'],\n).mark_area().encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q')\n).properties(\n    title=\"Movies runtime\"\n)\n\n\n\n\n\n\n\n\n\n2.1.4 Grouped Density plot\nWe can also compare distributions across groups by splitting the KDE by a categorical variable using the groupby parameter. This helps us see how the distribution differs between categories, such as genres.\n\n\nCode\nselection = alt.selection_single(fields=['Major_Genre'], bind='legend')\n\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    groupby=['Major_Genre'],\n    as_=['Running_Time_min', 'density'],\n).mark_area(opacity=0.5).encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q', stack=None),\n    alt.Color('Major_Genre'),\n    opacity=alt.condition(selection, \n        alt.value(1), \n        alt.value(0.05)\n    )\n).add_selection(\n    selection\n).properties(\n    title=\"Movies Runtime by Genre (Interactive Filter)\"\n).interactive()\n\n\n\n\n\n\n\nThe transparency (opacity=0.5) allows us to observe overlapping distributions and ensures that small density areas are not completely hidden behind larger ones.\nFrom this plot, we can observe, for example, that Drama movies have runtimes nearly as long as the longest Adventure movies, even though their overall distributions differ."
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "href": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.2 Bivariate Analysis: Categorical vs Quantitative",
    "text": "2.2 Bivariate Analysis: Categorical vs Quantitative\nBivariate analysis examines the relationship between two variables. In this case, we focus on one categorical variable (e.g., genre) and one quantitative variable (e.g., revenue), which is a very common scenario in exploratory data analysis.\nThis type of analysis is useful to: - Compare average or median values across categories. - Detect outliers or high-variance groups. - Understand distributional differences across categories.\nBelow are several effective visualizations for this analysis.\n\n2.2.1 Basic Barchart\nBar charts are effective for comparing aggregated values (like the mean) across different groups. However, they hide the distribution and variation within each group.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Average Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\nThis bar chart shows the mean Worldwide Gross per genre. It is useful for identifying which genres are more profitable on average, but does not show how spread out the data is.\n\n\n2.2.2 Tick Plot\nTo visualize individual data points, we use a tick plot. This helps uncover variability within genres and detect outliers.\n\n\nCode\nalt.Chart(movies_cleaned).mark_tick().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\"),\n    alt.Tooltip('Title:N')\n).properties(\n    title=\"Individual Gross per Movie by Genre\"\n)\n\n\n\n\n\n\n\n\n\n2.2.3 Heatmaps\nHeatmaps can summarize the frequency of data points across both axes (quantitative and categorical) using color intensity. It‚Äôs particularly useful for spotting patterns without getting overwhelmed by individual points.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Worldwide_Gross',bin=alt.Bin(maxbins=100)),\n    alt.Y(\"Major_Genre\"),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Heatmap of Movie Counts by Gross and Genre\"\n)\n\n\n\n\n\n\n\nThis heatmap shows how frequently movies from each genre fall into different revenue ranges.\n\n\n2.2.4 Boxplot\nBoxplots are useful for comparing distributions across categories and identifying outliers. Boxplots summarize a distribution using five statistics:\n\nMedian (Q2)\nFirst Quartile (Q1)\nThird Quartile (Q3)\nLower Whisker (Q1 - 1.5 √ó IQR)\nUpper Whisker (Q3 + 1.5 √ó IQR)\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Boxplot of Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\n\n2.2.5 Side-by-side: Boxplot and Bar Chart\nTo contrast aggregated values (bar chart) with the full distribution (boxplot), we can display them together:\n\n\nCode\nbar = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox = alt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox | bar\n\n\n\n\n\n\n\nThis comparison reveals whether the mean is a good representative of the genre, or whether the data is skewed or contains outliers that affect the average"
  },
  {
    "objectID": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "href": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.3 Bivariate Analysis: Quantitative vs Quantitative",
    "text": "2.3 Bivariate Analysis: Quantitative vs Quantitative\nWhen analyzing two quantitative (numerical) variables simultaneously, we aim to discover possible relationships, trends, or correlations. This type of bivariate analysis can reveal whether increases in one variable are associated with increases or decreases in another (positive or negative correlation), or if there‚Äôs no relationship at all. The most common and intuitive visualization for this is the scatterplot.\n\n2.3.1 Scatterplots\nScatter plots are effective visualizations for exploring two-dimensional distributions, allowing us to identify patterns, trends, clusters, or outliers.\nLet‚Äôs start by visualizing how movies are rated across two popular online platforms:\n\nIMDb\n\nRotten Tomatoes\n\nAre movies rated similarly on different platforms?\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('IMDB_Rating'),\n    alt.Y('Rotten_Tomatoes_Rating')\n).properties(\n    title=\"IMDB vs Rotten Tomatoes Ratings\"\n)\n\n\n\n\n\n\n\n\n\n2.3.2 Scatterplot Saturation\nScatterplots can become saturated when too many points overlap in a small area of the chart, making it difficult to distinguish dense regions from sparse ones. For example, when plotting financial variables like production budget versus worldwide gross:\n\n\nCode\nsaturated = alt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('Production_Budget'),\n    alt.Y('Worldwide_Gross')\n).properties(\n    title=\"Saturated Scatterplot: Budget vs Gross\"\n)\nsaturated\n\n\n\n\n\n\n\n\n\n2.3.3 Using Binned Heatmap to Reduce Saturation\nTo address saturation, we can bin both variables and use a heatmap where the color intensity represents the number of movies that fall into each rectangular region of the grid. This makes dense areas more interpretable\n\n\nCode\nheatmap_scatter = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Production_Budget', bin=alt.Bin(maxbins=60)),\n    alt.Y('Worldwide_Gross', bin=alt.Bin(maxbins=60)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Binned Heatmap: Budget vs Gross\"\n)\nheatmap_scatter\n\n\n\n\n\n\n\n\n\n2.3.4 Side-by-side Comparison\nCompare the raw scatterplot with the heatmap representation:\n\n\nCode\nsaturated | heatmap_scatter"
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "href": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.4 Bivariate Analysis: Categorical vs Categorical",
    "text": "2.4 Bivariate Analysis: Categorical vs Categorical\nWhen working with two categorical variables, bivariate analysis helps us understand how categories from one variable relate or are distributed across the other. For example, we might want to know how different movie genres are rated according to the MPAA rating system. Visualization techniques like grouped bar charts and faceted plots can reveal patterns, associations, or class imbalances.\n\n2.4.1 Basic Faceted Bar Chart\nWe begin by exploring how movies are rated (MPAA_Rating) across different genres (Major_Genre). A faceted bar chart allows us to visualize this relationship by plotting a bar chart per genre, helping to identify genre-specific rating distributions.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre'\n)\n\n\n\n\n\n\n\n\n\n2.4.2 Vertical Faceting for Alignment\nFaceting horizontally can make comparisons across genres harder when the x-axis is misaligned. By specifying columns=1, we lay out the facets vertically, making it easier to compare counts across genres.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=1\n)\n\n\n\n\n\n\n\n\n\n2.4.3 Dependent vs Independent Axis Scaling\nBy default, facet plots share the same x-axis scale (dependent scale), which allows for easier comparison across panels. However, when the number of observations varies greatly between genres, this shared scale can compress some charts.\nWe can instead use independent x-axis scaling for each facet. This highlights the relative distribution within each genre.\n\n\nCode\nshared_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n)\n\nindependent_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n).resolve_scale(x='independent')\n\nshared_scale | independent_scale\n\n\n\n\n\n\n\nThe left panel (shared scale) makes absolute comparisons between genres, while the right panel (independent scale) makes within-genre comparisons more readable.\n\n\n2.4.4 Heatmaps\nHeatmaps are effective for visualizing the relationship between two categorical variables when the goal is to display counts or frequency of occurrences. They map the number of observations to color, providing an intuitive view of which category pairs are most or least common.\nWe can enhance this basic representation by also using marker size, combining both color intensity and circle area to represent counts more effectively. This dual encoding can improve interpretation, especially when printed in grayscale or when there are subtle color differences.\n\n\nCode\nheatmap_color = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()')\n).properties(\n    title=\"Heatmap with Color (Count of Movies)\"\n)\n\nheatmap_size = alt.Chart(movies_cleaned).mark_circle().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()'),\n    alt.Size('count()')\n).properties(\n    title=\"Heatmap with Color + Size (Count of Movies)\"\n)\n\nheatmap_color | heatmap_size"
  },
  {
    "objectID": "eda.html#multivariate-analysis",
    "href": "eda.html#multivariate-analysis",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.5 Multivariate Analysis",
    "text": "2.5 Multivariate Analysis\nMultivariate analysis helps us understand the interactions and relationships among multiple variables simultaneously. In the context of numerical features, it is useful to explore pairwise distributions, correlations, and detect potential clusters or anomalies.\nWhen the number of variables is large, repeated charts such as histograms or scatter plot matrices help us summarize patterns efficiently and consistently across all numerical dimensions.\n\n2.5.1 Repeated Histograms for Numerical Columns\nWe first identify and isolate all numerical columns from the dataset. Then we repeat a histogram for each of these columns to understand the individual distributions. This overview is helpful to detect skewness, outliers, or binning decisions that affect how data is grouped visually.\n\n\nCode\n# Select only numerical columns\nnumerical_columns = movies_cleaned.select_dtypes('number').columns.tolist()\n\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X(alt.repeat(),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    width=150,\n    height=150\n).repeat(\n    numerical_columns,\n    columns=4\n)\n\n\n\n\n\n\n\n\n\n2.5.2 Scatter Plot Matrix (Pairplot)\nA scatter plot matrix shows the pairwise relationships between all numerical variables. This is a common exploratory tool to detect:\n\nCorrelations between variables\nOutliers or clusters\nRelationships useful for prediction models (e.g., to predict rating or budget)\n\nWe focus especially on the plots below the diagonal, as they are not duplicated.\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='quantitative'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n)\n\n\n\n\n\n\n\n\n\n2.5.3 Heatmap Matrix\nWhen scatter plots become too saturated (many overlapping points), heatmaps offer a better alternative by binning the numeric values and encoding the count in color intensity.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X(alt.repeat('column'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y(alt.repeat('row'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n).resolve_scale(\n    color='independent'\n)\n\n\n\n\n\n\n\nTo gain deeper insights into the dataset, it‚Äôs important to analyze how numerical variables behave across different categories. This type of multivariate analysis allows us to:\n\nCompare distributions across categories\nDetect outliers within categories\nObserve central tendency (median, quartiles) and spread (range, IQR)\n\nBoxplots are particularly effective for this purpose. In the following visualizations, we explore these relationships by repeating plots across combinations of categorical and numerical features.\n\n\n2.5.4 Filter Categorical Columns\nFirst, we select the relevant categorical columns, excluding identifiers and text-heavy variables like movie titles or director names.\n\n\nCode\ncategorical_columns =  movies_cleaned.select_dtypes('object').columns.to_list()\n\ncategorical_columns_remove = ['Title','Release_Date','Distributor','Director']\n\ncategorical_filtered = [col for col in categorical_columns if col not in categorical_columns_remove]\n\n\n\n\n2.5.5 Repeated Boxplots: Categorical vs Numerical\nWe repeat boxplots using combinations of categorical (rows) and numerical (columns) features. This matrix layout gives a clear visual overview of how numerical values are distributed within each category.\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=numerical_columns,\n    row=categorical_filtered\n)\n\n\n\n\n\n\n\n\n\n2.5.6 Faceted Boxplots\nFor more focused analysis, we can facet the boxplots using a specific categorical variable like MPAA_Rating, and repeat the chart by different categorical rows. This lets us keep the numerical axis fixed while comparing how categories vary across different classes (e.g., movie ratings).\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Running_Time_min', type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).facet(\n    column='MPAA_Rating'\n).repeat(\n    row=categorical_filtered\n)"
  },
  {
    "objectID": "regresion_hogares.html",
    "href": "regresion_hogares.html",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Este documento presenta un modelo de regresi√≥n lineal para predecir el decil socioecon√≥mico de hogares ecuatorianos utilizando datos de la Encuesta de Condiciones de Vida 2018.\nObjetivo: Desarrollar un modelo predictivo que estime el nivel socioecon√≥mico basado en caracter√≠sticas del hogar y ubicaci√≥n geogr√°fica.\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurar matplotlib\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"=== SISTEMA DE REGRESI√ìN - DECILES SOCIOECON√ìMICOS ===\")\n\n\n=== SISTEMA DE REGRESI√ìN - DECILES SOCIOECON√ìMICOS ===\n\n\n\n\nCode\n# Cargar datos principales\ndf = pd.read_csv('DATOS_REGRESSION/hogares_rs18.csv', encoding='utf-8')\n\nprint(f\"Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\nprint(f\"Columnas disponibles: {list(df.columns)}\")\n\n\nDatos cargados: 2565433 filas, 29 columnas\nColumnas disponibles: ['s1_id06_cod', 's1_id06_des', 's1_id05_cod', 's1_id05_des', 's1_id04_cod', 's1_id04_des', 's1_id03', 's3_vi02', 's3_vi05_est', 's3_vi04_est', 's3_vi03_est', 's3_vi05', 's3_vi04', 's3_vi03', 's3_vi01', 's4_ho08', 's4_ho17', 's4_ho16', 's4_ho06', 's4_ho01', 's4_ho19', 's4_ho12', 's4_ho21', 's4_ho22', 'decil', 'tipo_pob_rs18', 'tot_hogares', 'tot_nucleos', 'tot_personas']\n\n\n\n\nCode\n# Diccionario de variables conocidas\nvariable_mapping = {\n    'decil': 'Decil socioecon√≥mico del hogar (1=m√°s pobre, 10=m√°s rico)',\n    'tipo_pob_rs18': 'Tipo de pobreza del hogar',\n    'tot_hogares': 'Total de hogares en el √°rea',\n    'tot_nucleos': 'Total de n√∫cleos familiares',\n    'tot_personas': 'Total de personas en el hogar',\n    's1_id03': 'C√≥digo de √°rea (1=Urbano, 0=Rural)',\n    's3_vi01': 'Tipo de vivienda',\n    's3_vi03': 'Material predominante del piso',\n    's3_vi04': 'Material predominante del techo',\n    's3_vi05': 'Material predominante de las paredes',\n    's4_ho01': 'Procedencia principal del agua',\n    's4_ho06': 'Tipo de servicio el√©ctrico',\n    's4_ho08': 'Tipo de servicio higi√©nico',\n    's4_ho12': 'Servicio telef√≥nico convencional',\n    's4_ho16': 'Eliminaci√≥n de la basura',\n    's4_ho17': 'Combustible para cocinar'\n}\n\nprint(\"=== DICCIONARIO DE VARIABLES ===\")\nfor var, desc in variable_mapping.items():\n    print(f\"{var}: {desc}\")\n\n\n=== DICCIONARIO DE VARIABLES ===\ndecil: Decil socioecon√≥mico del hogar (1=m√°s pobre, 10=m√°s rico)\ntipo_pob_rs18: Tipo de pobreza del hogar\ntot_hogares: Total de hogares en el √°rea\ntot_nucleos: Total de n√∫cleos familiares\ntot_personas: Total de personas en el hogar\ns1_id03: C√≥digo de √°rea (1=Urbano, 0=Rural)\ns3_vi01: Tipo de vivienda\ns3_vi03: Material predominante del piso\ns3_vi04: Material predominante del techo\ns3_vi05: Material predominante de las paredes\ns4_ho01: Procedencia principal del agua\ns4_ho06: Tipo de servicio el√©ctrico\ns4_ho08: Tipo de servicio higi√©nico\ns4_ho12: Servicio telef√≥nico convencional\ns4_ho16: Eliminaci√≥n de la basura\ns4_ho17: Combustible para cocinar\n\n\n\n\n\n\n\nCode\nprint(\"=== EXPLORACI√ìN INICIAL ===\")\nprint(f\"Forma del dataset: {df.shape}\")\nprint(f\"\\nPrimeras 5 filas:\")\ndf.head()\n\n\n=== EXPLORACI√ìN INICIAL ===\nForma del dataset: (2565433, 29)\n\nPrimeras 5 filas:\n\n\n\n\n\n\n\n\n\ns1_id06_cod\ns1_id06_des\ns1_id05_cod\ns1_id05_des\ns1_id04_cod\ns1_id04_des\ns1_id03\ns3_vi02\ns3_vi05_est\ns3_vi04_est\n...\ns4_ho01\ns4_ho19\ns4_ho12\ns4_ho21\ns4_ho22\ndecil\ntipo_pob_rs18\ntot_hogares\ntot_nucleos\ntot_personas\n\n\n\n\n0\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n8\nNo pobre\n2\n3\n9\n\n\n1\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n9\nNo pobre\n5\n5\n22\n\n\n2\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n10\nNo pobre\n1\n1\n2\n\n\n3\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n2\n4\nNo pobre\n1\n2\n7\n\n\n4\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n2\n5\nNo pobre\n2\n3\n12\n\n\n\n\n5 rows √ó 29 columns\n\n\n\n\n\nCode\n# Estad√≠sticas descriptivas de variables clave\nprint(\"=== ESTAD√çSTICAS DESCRIPTIVAS ===\")\nkey_columns = ['decil', 'tot_hogares', 'tot_personas', 's3_vi01', 'tipo_pob_rs18']\nexisting_cols = [col for col in key_columns if col in df.columns]\nif existing_cols:\n    df[existing_cols].describe()\nelse:\n    print(\"No se encontraron las columnas esperadas\")\n\n\n=== ESTAD√çSTICAS DESCRIPTIVAS ===\n\n\n\n\nCode\n# Verificar valores faltantes\nprint(\"=== VALORES FALTANTES ===\")\nmissing_data = df.isnull().sum()\nmissing_percent = (missing_data / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Columna': missing_data.index,\n    'Valores_Faltantes': missing_data.values,\n    'Porcentaje': missing_percent.values\n}).sort_values('Valores_Faltantes', ascending=False)\n\nprint(missing_df.head(10))\n\n\n=== VALORES FALTANTES ===\n          Columna  Valores_Faltantes  Porcentaje\n0     s1_id06_cod                  0         0.0\n15        s4_ho08                  0         0.0\n27    tot_nucleos                  0         0.0\n26    tot_hogares                  0         0.0\n25  tipo_pob_rs18                  0         0.0\n24          decil                  0         0.0\n23        s4_ho22                  0         0.0\n22        s4_ho21                  0         0.0\n21        s4_ho12                  0         0.0\n20        s4_ho19                  0         0.0\n\n\n\n\n\n\n\nCode\nprint(\"=== DEFINIENDO VARIABLE OBJETIVO ===\")\n# Variable objetivo: decil socioecon√≥mico\ntarget_col = 'decil'\nif target_col in df.columns:\n    print(f\"Variable objetivo: {target_col}\")\n    print(f\"Distribuci√≥n de deciles:\")\n    print(df[target_col].value_counts().sort_index())\n    \n    # Visualizar distribuci√≥n\n    plt.figure(figsize=(10, 6))\n    df[target_col].hist(bins=10, edgecolor='black', alpha=0.7, color='skyblue')\n    plt.title('Distribuci√≥n de Deciles Socioecon√≥micos', fontsize=14, fontweight='bold')\n    plt.xlabel('Decil')\n    plt.ylabel('Frecuencia')\n    plt.grid(alpha=0.3)\n    plt.show()\nelse:\n    print(f\"ERROR: Columna {target_col} no encontrada\")\n\n\n=== DEFINIENDO VARIABLE OBJETIVO ===\nVariable objetivo: decil\nDistribuci√≥n de deciles:\n1     382465\n2     499175\n3     403812\n4     334334\n5     263928\n6     228965\n7     192018\n8     143883\n9      86887\n10     29966\nName: decil, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"=== SELECCIONANDO Y CLASIFICANDO FEATURES ===\")\n# Primero ver todas las columnas disponibles\nprint(\"Columnas disponibles en el dataset:\")\nprint(list(df.columns))\n\n# Clasificar features por tipo de datos\nnumerical_features = [\n    'tot_hogares',   # Total de hogares\n    'tot_nucleos',   # Total de n√∫cleos familiares\n    'tot_personas'   # Total de personas\n]\n\ncategorical_features = [\n    's3_vi01',       # Tipo de vivienda\n    's3_vi03',       # Material de piso\n    's3_vi04',       # Material de techo  \n    's3_vi05',       # Material de paredes\n    's4_ho01',       # Procedencia del agua\n    's4_ho06',       # Servicio el√©ctrico\n    's4_ho08',       # Servicio higi√©nico\n    's4_ho12',       # Tel√©fono convencional\n    's4_ho16',       # Eliminaci√≥n de basura\n    's4_ho17'        # Combustible para cocinar\n]\n\nall_features = numerical_features + categorical_features\n\n# Verificar qu√© columnas existen\navailable_numerical = [col for col in numerical_features if col in df.columns]\navailable_categorical = [col for col in categorical_features if col in df.columns]\navailable_features = available_numerical + available_categorical\n\nprint(f\"Features num√©ricas disponibles: {available_numerical}\")\nprint(f\"Features categ√≥ricas disponibles: {available_categorical}\")\nprint(f\"Total features: {len(available_features)}\")\n\n\n=== SELECCIONANDO Y CLASIFICANDO FEATURES ===\nColumnas disponibles en el dataset:\n['s1_id06_cod', 's1_id06_des', 's1_id05_cod', 's1_id05_des', 's1_id04_cod', 's1_id04_des', 's1_id03', 's3_vi02', 's3_vi05_est', 's3_vi04_est', 's3_vi03_est', 's3_vi05', 's3_vi04', 's3_vi03', 's3_vi01', 's4_ho08', 's4_ho17', 's4_ho16', 's4_ho06', 's4_ho01', 's4_ho19', 's4_ho12', 's4_ho21', 's4_ho22', 'decil', 'tipo_pob_rs18', 'tot_hogares', 'tot_nucleos', 'tot_personas']\nFeatures num√©ricas disponibles: ['tot_hogares', 'tot_nucleos', 'tot_personas']\nFeatures categ√≥ricas disponibles: ['s3_vi01', 's3_vi03', 's3_vi04', 's3_vi05', 's4_ho01', 's4_ho06', 's4_ho08', 's4_ho12', 's4_ho16', 's4_ho17']\nTotal features: 13\n\n\n\n\n\n\n\nCode\nprint(\"=== PREPARANDO DATOS PARA EL MODELO ===\")\n# Crear dataset limpio\nmodel_data = df[available_features + [target_col]].copy()\n\n# Eliminar filas con valores faltantes\ninitial_rows = len(model_data)\nmodel_data = model_data.dropna()\nfinal_rows = len(model_data)\n\nprint(f\"Filas iniciales: {initial_rows}\")\nprint(f\"Filas despu√©s de limpiar: {final_rows}\")\nprint(f\"Filas eliminadas: {initial_rows - final_rows}\")\n\n# Separar features y target\nX = model_data[available_features]\ny = model_data[target_col]\n\nprint(f\"Forma final de X: {X.shape}\")\nprint(f\"Forma final de y: {y.shape}\")\nprint(f\"Features num√©ricas: {available_numerical}\")\nprint(f\"Features categ√≥ricas: {available_categorical}\")\n\n\n=== PREPARANDO DATOS PARA EL MODELO ===\nFilas iniciales: 2565433\nFilas despu√©s de limpiar: 2565433\nFilas eliminadas: 0\nForma final de X: (2565433, 13)\nForma final de y: (2565433,)\nFeatures num√©ricas: ['tot_hogares', 'tot_nucleos', 'tot_personas']\nFeatures categ√≥ricas: ['s3_vi01', 's3_vi03', 's3_vi04', 's3_vi05', 's4_ho01', 's4_ho06', 's4_ho08', 's4_ho12', 's4_ho16', 's4_ho17']\n\n\n\n\nCode\n# Estad√≠sticas de las features\nprint(\"=== ESTAD√çSTICAS DE FEATURES ===\")\nif len(available_features) &gt; 0:\n    X.describe()\nelse:\n    print(\"No hay features disponibles para mostrar estad√≠sticas\")\n\n\n=== ESTAD√çSTICAS DE FEATURES ===\n\n\n\n\n\n\n\nCode\nprint(\"=== DIVIDIENDO DATOS ===\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Entrenamiento: {X_train.shape[0]} muestras\")\nprint(f\"Prueba: {X_test.shape[0]} muestras\")\nprint(f\"Estad√≠sticas del target en entrenamiento:\")\nprint(f\"Media: {y_train.mean():.2f}\")\nprint(f\"Desviaci√≥n est√°ndar: {y_train.std():.2f}\")\n\n\n=== DIVIDIENDO DATOS ===\nEntrenamiento: 2052346 muestras\nPrueba: 513087 muestras\nEstad√≠sticas del target en entrenamiento:\nMedia: 3.98\nDesviaci√≥n est√°ndar: 2.36\n\n\n\n\n\n\n\nCode\nprint(\"=== CREANDO PIPELINE DE REGRESI√ìN ===\")\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Crear pipeline de preprocesamiento correcto\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), available_numerical),\n        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), available_categorical)\n    ])\n\n# Pipeline completo con preprocesamiento adecuado\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])\n\n# Entrenar modelo\nprint(\"Entrenando modelo LinearRegression con preprocesamiento correcto...\")\nprint(f\"Variables num√©ricas: {available_numerical}\")\nprint(f\"Variables categ√≥ricas: {available_categorical}\")\npipeline.fit(X_train, y_train)\nprint(\"‚úì Modelo entrenado exitosamente\")\n\n\n=== CREANDO PIPELINE DE REGRESI√ìN ===\nEntrenando modelo LinearRegression con preprocesamiento correcto...\nVariables num√©ricas: ['tot_hogares', 'tot_nucleos', 'tot_personas']\nVariables categ√≥ricas: ['s3_vi01', 's3_vi03', 's3_vi04', 's3_vi05', 's4_ho01', 's4_ho06', 's4_ho08', 's4_ho12', 's4_ho16', 's4_ho17']\n‚úì Modelo entrenado exitosamente\n\n\n\n\n\n\n\nCode\nprint(\"=== REALIZANDO PREDICCIONES ===\")\ny_pred_train = pipeline.predict(X_train)\ny_pred_test = pipeline.predict(X_test)\n\nprint(f\"Predicciones generadas para {len(y_pred_test)} muestras de prueba\")\nprint(f\"Rango de predicciones: [{y_pred_test.min():.2f}, {y_pred_test.max():.2f}]\")\nprint(f\"Rango real: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n\n\n=== REALIZANDO PREDICCIONES ===\nPredicciones generadas para 513087 muestras de prueba\nRango de predicciones: [-45.74, 86.93]\nRango real: [1.00, 10.00]\n\n\n\n\n\n\n\nCode\nprint(\"=== EVALUACI√ìN DEL MODELO ===\")\n# M√©tricas en conjunto de entrenamiento\nmse_train = mean_squared_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mse_train)\nr2_train = r2_score(y_train, y_pred_train)\n\n# M√©tricas en conjunto de prueba\nmse_test = mean_squared_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mse_test)\nr2_test = r2_score(y_test, y_pred_test)\n\n# Crear DataFrame con m√©tricas\nmetricas = pd.DataFrame({\n    'M√©trica': ['MSE', 'RMSE', 'R¬≤ Score'],\n    'Entrenamiento': [mse_train, rmse_train, r2_train],\n    'Prueba': [mse_test, rmse_test, r2_test],\n    'Interpretaci√≥n': [\n        f'Error cuadr√°tico medio: {mse_test:.3f}',\n        f'Error promedio: ¬±{rmse_test:.2f} deciles',\n        f'Varianza explicada: {r2_test*100:.1f}%'\n    ]\n})\n\nprint(metricas.to_string(index=False))\n\n\n=== EVALUACI√ìN DEL MODELO ===\n M√©trica  Entrenamiento   Prueba                Interpretaci√≥n\n     MSE       3.466656 3.385502 Error cuadr√°tico medio: 3.386\n    RMSE       1.861896 1.839973 Error promedio: ¬±1.84 deciles\nR¬≤ Score       0.378183 0.392704     Varianza explicada: 39.3%\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 8))\nplt.scatter(y_test, y_pred_test, alpha=0.6, color='blue', s=20)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Deciles Reales')\nplt.ylabel('Deciles Predichos')\nplt.title('Valores Reales vs Predichos - Deciles Socioecon√≥micos', fontsize=14, fontweight='bold')\nplt.grid(alpha=0.3)\n\n# A√±adir m√©tricas al gr√°fico\nplt.text(0.05, 0.95, f'R¬≤ = {r2_test:.3f}\\nRMSE = {rmse_test:.2f}', \n         transform=plt.gca().transAxes, fontsize=12, \n         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(12, 5))\n\n# Subplot 1: Histograma de errores\nplt.subplot(1, 2, 1)\nerrors = y_test - y_pred_test\nplt.hist(errors, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')\nplt.xlabel('Error (Real - Predicho)')\nplt.ylabel('Frecuencia')\nplt.title('Distribuci√≥n de Errores', fontweight='bold')\nplt.axvline(x=0, color='red', linestyle='--', alpha=0.8)\nplt.grid(alpha=0.3)\n\n# Subplot 2: Errores vs valores predichos\nplt.subplot(1, 2, 2)\nplt.scatter(y_pred_test, errors, alpha=0.6, color='green', s=20)\nplt.xlabel('Valores Predichos')\nplt.ylabel('Error (Real - Predicho)')\nplt.title('Errores vs Valores Predichos', fontweight='bold')\nplt.axhline(y=0, color='red', linestyle='--', alpha=0.8)\nplt.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"=== GENERANDO CURVA DE APRENDIZAJE ===\")\ntrain_sizes, train_scores, val_scores = learning_curve(\n    pipeline, X_train, y_train, cv=5, \n    train_sizes=np.linspace(0.1, 1.0, 10),\n    scoring='r2', random_state=42\n)\n\n# Calcular medias y desviaciones est√°ndar\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_sizes, train_mean, 'o-', color='blue', label='Entrenamiento')\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\nplt.plot(train_sizes, val_mean, 'o-', color='red', label='Validaci√≥n')\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n\nplt.xlabel('Tama√±o del Conjunto de Entrenamiento')\nplt.ylabel('R¬≤ Score')\nplt.title('Curva de Aprendizaje - Regresi√≥n de Deciles', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n=== GENERANDO CURVA DE APRENDIZAJE ===\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"=== AN√ÅLISIS DE IMPORTANCIA DE FEATURES ===\")\n# Obtener nombres de features despu√©s del preprocesamiento\nfeature_names = (available_numerical + \n                list(pipeline.named_steps['preprocessor']\n                    .named_transformers_['cat']\n                    .get_feature_names_out(available_categorical)))\n\ncoefficients = pipeline.named_steps['regressor'].coef_\n\n# Crear DataFrame con importancias\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'coefficient': coefficients,\n    'abs_coefficient': np.abs(coefficients)\n}).sort_values('abs_coefficient', ascending=False)\n\n# Mapeo de nombres descriptivos para interpretaci√≥n\nfeature_descriptions = {\n    'tot_personas': 'Total de personas en el hogar',\n    'tot_nucleos': 'Total de n√∫cleos familiares', \n    'tot_hogares': 'Total de hogares en el √°rea'\n}\n\nprint(\"Top 15 features m√°s importantes:\")\nfor _, row in feature_importance.head(15).iterrows():\n    var_name = row['feature']\n    coef = row['coefficient']\n    \n    # Descripci√≥n m√°s clara para variables categ√≥ricas\n    if var_name in feature_descriptions:\n        description = feature_descriptions[var_name]\n    elif 's3_vi03' in var_name:\n        description = f'Material del piso (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's3_vi04' in var_name:\n        description = f'Material del techo (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's3_vi05' in var_name:\n        description = f'Material de paredes (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho01' in var_name:\n        description = f'Procedencia del agua (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho06' in var_name:\n        description = f'Servicio el√©ctrico (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho08' in var_name:\n        description = f'Servicio higi√©nico (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho12' in var_name:\n        description = f'Tel√©fono fijo (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho16' in var_name:\n        description = f'Recolecci√≥n de basura (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho17' in var_name:\n        description = f'Combustible para cocinar (categor√≠a {var_name.split(\"_\")[-1]})'\n    else:\n        description = 'Variable del hogar'\n    \n    print(f\"{var_name:25} | {coef:8.3f} | {description}\")\n\n\n=== AN√ÅLISIS DE IMPORTANCIA DE FEATURES ===\nTop 15 features m√°s importantes:\ntot_personas              |   -2.653 | Total de personas en el hogar\ns3_vi04_7                 |   -1.847 | Material del techo (categor√≠a 7)\ntot_nucleos               |    1.704 | Total de n√∫cleos familiares\ns3_vi04_6                 |   -1.578 | Material del techo (categor√≠a 6)\ns3_vi04_4                 |   -1.436 | Material del techo (categor√≠a 4)\ns3_vi04_5                 |   -1.260 | Material del techo (categor√≠a 5)\ns3_vi04_8                 |   -1.080 | Material del techo (categor√≠a 8)\ns4_ho12_5                 |   -0.965 | Tel√©fono fijo (categor√≠a 5)\ns4_ho12_6                 |   -0.955 | Tel√©fono fijo (categor√≠a 6)\ns4_ho12_3                 |   -0.897 | Tel√©fono fijo (categor√≠a 3)\ntot_hogares               |    0.859 | Total de hogares en el √°rea\ns4_ho17_3                 |    0.799 | Combustible para cocinar (categor√≠a 3)\ns4_ho01_3                 |   -0.794 | Procedencia del agua (categor√≠a 3)\ns4_ho17_2                 |   -0.750 | Combustible para cocinar (categor√≠a 2)\ns4_ho01_4                 |   -0.713 | Procedencia del agua (categor√≠a 4)\n\n\n\n\nCode\n# Visualizar importancia de features (solo top 15)\nplt.figure(figsize=(12, 8))\ntop_features = feature_importance.head(15)  # Solo las 15 m√°s importantes\ncolors = ['red' if coef &lt; 0 else 'green' for coef in top_features['coefficient']]\nplt.barh(range(len(top_features)), top_features['coefficient'], color=colors)\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Coeficiente')\nplt.title('Top 15 Features M√°s Importantes (Coeficientes de Regresi√≥n)', fontsize=14, fontweight='bold')\nplt.grid(axis='x', alpha=0.3)\n\n# A√±adir l√≠nea en x=0\nplt.axvline(x=0, color='black', linestyle='-', alpha=0.8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"=== AN√ÅLISIS POR DECILES ===\")\n# Crear DataFrame con resultados por decil\nresults_by_decil = pd.DataFrame({\n    'decil_real': y_test,\n    'decil_predicho': y_pred_test,\n    'error': y_test - y_pred_test,\n    'error_abs': np.abs(y_test - y_pred_test)\n})\n\n# Estad√≠sticas por decil real\ndecil_stats = results_by_decil.groupby('decil_real').agg({\n    'decil_predicho': ['mean', 'std'],\n    'error': ['mean', 'std'],\n    'error_abs': 'mean'\n}).round(3)\n\nprint(\"Estad√≠sticas por decil real:\")\nprint(decil_stats)\n\n\n=== AN√ÅLISIS POR DECILES ===\nEstad√≠sticas por decil real:\n           decil_predicho         error        error_abs\n                     mean    std   mean    std      mean\ndecil_real                                              \n1                   2.513  1.048 -1.513  1.048     1.569\n2                   3.299  1.036 -1.299  1.036     1.379\n3                   3.798  1.047 -0.798  1.047     1.038\n4                   4.171  1.061 -0.171  1.061     0.841\n5                   4.461  1.061  0.539  1.061     0.958\n6                   4.724  1.095  1.276  1.095     1.381\n7                   4.989  1.117  2.011  1.117     2.045\n8                   5.334  1.338  2.666  1.338     2.721\n9                   5.713  1.403  3.287  1.403     3.357\n10                  6.107  1.227  3.893  1.227     3.945\n\n\n\n\nCode\n# Visualizaci√≥n por deciles\nplt.figure(figsize=(12, 8))\n\n# Boxplot de predicciones por decil real\nplt.subplot(2, 2, 1)\nresults_by_decil.boxplot(column='decil_predicho', by='decil_real', ax=plt.gca())\nplt.title('Predicciones por Decil Real')\nplt.xlabel('Decil Real')\nplt.ylabel('Decil Predicho')\n\n# Boxplot de errores por decil real\nplt.subplot(2, 2, 2)\nresults_by_decil.boxplot(column='error_abs', by='decil_real', ax=plt.gca())\nplt.title('Error Absoluto por Decil Real')\nplt.xlabel('Decil Real')\nplt.ylabel('Error Absoluto')\n\n# Scatter plot con colores por decil\nplt.subplot(2, 2, 3)\nscatter = plt.scatter(results_by_decil['decil_real'], results_by_decil['decil_predicho'], \n                     c=results_by_decil['decil_real'], cmap='viridis', alpha=0.6)\nplt.plot([1, 10], [1, 10], 'r--', alpha=0.8)\nplt.xlabel('Decil Real')\nplt.ylabel('Decil Predicho')\nplt.title('Predicciones Coloreadas por Decil')\nplt.colorbar(scatter)\n\n# Histograma de errores\nplt.subplot(2, 2, 4)\nplt.hist(results_by_decil['error'], bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel('Error (Real - Predicho)')\nplt.ylabel('Frecuencia')\nplt.title('Distribuci√≥n de Errores')\nplt.axvline(x=0, color='red', linestyle='--', alpha=0.8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl modelo de regresi√≥n lineal con preprocesamiento correcto muestra los siguientes resultados:\n\nR¬≤ Score (0.393): El modelo explica el 39.3% de la varianza en los deciles socioecon√≥micos\nRMSE (1.84): Error promedio de aproximadamente 1.8 deciles\n\nMejora significativa: El tratamiento correcto de variables categ√≥ricas mejor√≥ el rendimiento predictivo\n\n\n\n\nEl an√°lisis con preprocesamiento correcto revela patrones m√°s precisos:\n\nComposici√≥n del hogar:\n\nTotal de personas (coef: -2.65): Hogares m√°s grandes tienden a ser m√°s pobres\nTotal de n√∫cleos (coef: +1.70): M√°s n√∫cleos familiares indica mejor situaci√≥n econ√≥mica\n\nMaterial del techo (altamente predictivo):\n\nCategor√≠as espec√≠ficas (4, 5, 6, 7, 8) est√°n fuertemente asociadas con pobreza\nEl material del techo es el segundo factor m√°s importante despu√©s del tama√±o del hogar\n\nAcceso a servicios de comunicaci√≥n:\n\nTel√©fono fijo: Ciertas categor√≠as (ausencia o tipo b√°sico) predicen deciles m√°s bajos\nRefleja el acceso a servicios modernos de comunicaci√≥n\n\nInfraestructura de la vivienda:\n\nMateriales de construcci√≥n espec√≠ficos son marcadores claros de nivel socioecon√≥mico\n\n\n\n\n\n\nDeciles bajos (1-3): Mejor predicci√≥n, caracter√≠sticas m√°s homog√©neas\nDeciles medios (4-7): Mayor variabilidad en las predicciones\nDeciles altos (8-10): Tendencia a subestimar el nivel socioecon√≥mico\n\n\n\n\nEl modelo presenta algunas limitaciones importantes que deben considerarse al interpretar los resultados:\n\nVarianza no explicada: El modelo explica aproximadamente el 39.3% de la variabilidad en los deciles socioecon√≥micos, lo que significa que existe un 60.7% de varianza que no est√° siendo capturada. Esto sugiere que hay factores socioecon√≥micos importantes que no est√°n incluidos en nuestro conjunto de variables, como ingresos familiares, educaci√≥n, empleo, etc.\nSimplicidad del modelo lineal: La regresi√≥n lineal asume relaciones lineales entre las variables, pero en la realidad socioecon√≥mica, las relaciones suelen ser m√°s complejas y no lineales.\nVariables seleccionadas limitadas: De las 29 caracter√≠sticas disponibles en el dataset, solo utilizamos 13 variables. Es posible que otras caracter√≠sticas como ubicaci√≥n geogr√°fica (provincia, cant√≥n) o variables adicionales de la vivienda podr√≠an mejorar la precisi√≥n del modelo.\n\n\n\n\n\n\nMaterial del techo es altamente predictivo: Las categor√≠as espec√≠ficas de materiales de construcci√≥n son marcadores claros de nivel socioecon√≥mico, siendo el segundo factor m√°s importante\nComposici√≥n del hogar es el factor m√°s cr√≠tico: Hogares con m√°s personas tienden a ser m√°s pobres, mientras que m√°s n√∫cleos familiares indica mejor situaci√≥n econ√≥mica\nServicios de comunicaci√≥n reflejan desigualdad: El acceso a tel√©fono fijo y su tipo son indicadores importantes del nivel socioecon√≥mico\nAplicaciones en pol√≠tica social: El modelo puede identificar hogares vulnerables bas√°ndose en caracter√≠sticas observables de la vivienda y composici√≥n familiar\n\n\n\n\nIdentificaci√≥n de vulnerabilidad: El modelo puede ayudar a identificar hogares en situaci√≥n de pobreza\nPlanificaci√≥n urbana: Priorizaci√≥n de inversiones en infraestructura"
  },
  {
    "objectID": "regresion_hogares.html#introducci√≥n",
    "href": "regresion_hogares.html#introducci√≥n",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Este documento presenta un modelo de regresi√≥n lineal para predecir el decil socioecon√≥mico de hogares ecuatorianos utilizando datos de la Encuesta de Condiciones de Vida 2018.\nObjetivo: Desarrollar un modelo predictivo que estime el nivel socioecon√≥mico basado en caracter√≠sticas del hogar y ubicaci√≥n geogr√°fica."
  },
  {
    "objectID": "regresion_hogares.html#carga-y-exploraci√≥n-del-dataset",
    "href": "regresion_hogares.html#carga-y-exploraci√≥n-del-dataset",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurar matplotlib\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"=== SISTEMA DE REGRESI√ìN - DECILES SOCIOECON√ìMICOS ===\")\n\n\n=== SISTEMA DE REGRESI√ìN - DECILES SOCIOECON√ìMICOS ===\n\n\n\n\nCode\n# Cargar datos principales\ndf = pd.read_csv('DATOS_REGRESSION/hogares_rs18.csv', encoding='utf-8')\n\nprint(f\"Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\nprint(f\"Columnas disponibles: {list(df.columns)}\")\n\n\nDatos cargados: 2565433 filas, 29 columnas\nColumnas disponibles: ['s1_id06_cod', 's1_id06_des', 's1_id05_cod', 's1_id05_des', 's1_id04_cod', 's1_id04_des', 's1_id03', 's3_vi02', 's3_vi05_est', 's3_vi04_est', 's3_vi03_est', 's3_vi05', 's3_vi04', 's3_vi03', 's3_vi01', 's4_ho08', 's4_ho17', 's4_ho16', 's4_ho06', 's4_ho01', 's4_ho19', 's4_ho12', 's4_ho21', 's4_ho22', 'decil', 'tipo_pob_rs18', 'tot_hogares', 'tot_nucleos', 'tot_personas']\n\n\n\n\nCode\n# Diccionario de variables conocidas\nvariable_mapping = {\n    'decil': 'Decil socioecon√≥mico del hogar (1=m√°s pobre, 10=m√°s rico)',\n    'tipo_pob_rs18': 'Tipo de pobreza del hogar',\n    'tot_hogares': 'Total de hogares en el √°rea',\n    'tot_nucleos': 'Total de n√∫cleos familiares',\n    'tot_personas': 'Total de personas en el hogar',\n    's1_id03': 'C√≥digo de √°rea (1=Urbano, 0=Rural)',\n    's3_vi01': 'Tipo de vivienda',\n    's3_vi03': 'Material predominante del piso',\n    's3_vi04': 'Material predominante del techo',\n    's3_vi05': 'Material predominante de las paredes',\n    's4_ho01': 'Procedencia principal del agua',\n    's4_ho06': 'Tipo de servicio el√©ctrico',\n    's4_ho08': 'Tipo de servicio higi√©nico',\n    's4_ho12': 'Servicio telef√≥nico convencional',\n    's4_ho16': 'Eliminaci√≥n de la basura',\n    's4_ho17': 'Combustible para cocinar'\n}\n\nprint(\"=== DICCIONARIO DE VARIABLES ===\")\nfor var, desc in variable_mapping.items():\n    print(f\"{var}: {desc}\")\n\n\n=== DICCIONARIO DE VARIABLES ===\ndecil: Decil socioecon√≥mico del hogar (1=m√°s pobre, 10=m√°s rico)\ntipo_pob_rs18: Tipo de pobreza del hogar\ntot_hogares: Total de hogares en el √°rea\ntot_nucleos: Total de n√∫cleos familiares\ntot_personas: Total de personas en el hogar\ns1_id03: C√≥digo de √°rea (1=Urbano, 0=Rural)\ns3_vi01: Tipo de vivienda\ns3_vi03: Material predominante del piso\ns3_vi04: Material predominante del techo\ns3_vi05: Material predominante de las paredes\ns4_ho01: Procedencia principal del agua\ns4_ho06: Tipo de servicio el√©ctrico\ns4_ho08: Tipo de servicio higi√©nico\ns4_ho12: Servicio telef√≥nico convencional\ns4_ho16: Eliminaci√≥n de la basura\ns4_ho17: Combustible para cocinar"
  },
  {
    "objectID": "regresion_hogares.html#exploraci√≥n-de-datos",
    "href": "regresion_hogares.html#exploraci√≥n-de-datos",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== EXPLORACI√ìN INICIAL ===\")\nprint(f\"Forma del dataset: {df.shape}\")\nprint(f\"\\nPrimeras 5 filas:\")\ndf.head()\n\n\n=== EXPLORACI√ìN INICIAL ===\nForma del dataset: (2565433, 29)\n\nPrimeras 5 filas:\n\n\n\n\n\n\n\n\n\ns1_id06_cod\ns1_id06_des\ns1_id05_cod\ns1_id05_des\ns1_id04_cod\ns1_id04_des\ns1_id03\ns3_vi02\ns3_vi05_est\ns3_vi04_est\n...\ns4_ho01\ns4_ho19\ns4_ho12\ns4_ho21\ns4_ho22\ndecil\ntipo_pob_rs18\ntot_hogares\ntot_nucleos\ntot_personas\n\n\n\n\n0\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n8\nNo pobre\n2\n3\n9\n\n\n1\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n9\nNo pobre\n5\n5\n22\n\n\n2\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n1\n10\nNo pobre\n1\n1\n2\n\n\n3\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n2\n4\nNo pobre\n1\n2\n7\n\n\n4\n10150\nCUENCA\n101\nCUENCA\n1\nAZUAY\n1\n1\n1\n1\n...\n1\n1\n1\n1\n2\n5\nNo pobre\n2\n3\n12\n\n\n\n\n5 rows √ó 29 columns\n\n\n\n\n\nCode\n# Estad√≠sticas descriptivas de variables clave\nprint(\"=== ESTAD√çSTICAS DESCRIPTIVAS ===\")\nkey_columns = ['decil', 'tot_hogares', 'tot_personas', 's3_vi01', 'tipo_pob_rs18']\nexisting_cols = [col for col in key_columns if col in df.columns]\nif existing_cols:\n    df[existing_cols].describe()\nelse:\n    print(\"No se encontraron las columnas esperadas\")\n\n\n=== ESTAD√çSTICAS DESCRIPTIVAS ===\n\n\n\n\nCode\n# Verificar valores faltantes\nprint(\"=== VALORES FALTANTES ===\")\nmissing_data = df.isnull().sum()\nmissing_percent = (missing_data / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Columna': missing_data.index,\n    'Valores_Faltantes': missing_data.values,\n    'Porcentaje': missing_percent.values\n}).sort_values('Valores_Faltantes', ascending=False)\n\nprint(missing_df.head(10))\n\n\n=== VALORES FALTANTES ===\n          Columna  Valores_Faltantes  Porcentaje\n0     s1_id06_cod                  0         0.0\n15        s4_ho08                  0         0.0\n27    tot_nucleos                  0         0.0\n26    tot_hogares                  0         0.0\n25  tipo_pob_rs18                  0         0.0\n24          decil                  0         0.0\n23        s4_ho22                  0         0.0\n22        s4_ho21                  0         0.0\n21        s4_ho12                  0         0.0\n20        s4_ho19                  0         0.0"
  },
  {
    "objectID": "regresion_hogares.html#definici√≥n-de-variable-objetivo-y-features",
    "href": "regresion_hogares.html#definici√≥n-de-variable-objetivo-y-features",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== DEFINIENDO VARIABLE OBJETIVO ===\")\n# Variable objetivo: decil socioecon√≥mico\ntarget_col = 'decil'\nif target_col in df.columns:\n    print(f\"Variable objetivo: {target_col}\")\n    print(f\"Distribuci√≥n de deciles:\")\n    print(df[target_col].value_counts().sort_index())\n    \n    # Visualizar distribuci√≥n\n    plt.figure(figsize=(10, 6))\n    df[target_col].hist(bins=10, edgecolor='black', alpha=0.7, color='skyblue')\n    plt.title('Distribuci√≥n de Deciles Socioecon√≥micos', fontsize=14, fontweight='bold')\n    plt.xlabel('Decil')\n    plt.ylabel('Frecuencia')\n    plt.grid(alpha=0.3)\n    plt.show()\nelse:\n    print(f\"ERROR: Columna {target_col} no encontrada\")\n\n\n=== DEFINIENDO VARIABLE OBJETIVO ===\nVariable objetivo: decil\nDistribuci√≥n de deciles:\n1     382465\n2     499175\n3     403812\n4     334334\n5     263928\n6     228965\n7     192018\n8     143883\n9      86887\n10     29966\nName: decil, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"=== SELECCIONANDO Y CLASIFICANDO FEATURES ===\")\n# Primero ver todas las columnas disponibles\nprint(\"Columnas disponibles en el dataset:\")\nprint(list(df.columns))\n\n# Clasificar features por tipo de datos\nnumerical_features = [\n    'tot_hogares',   # Total de hogares\n    'tot_nucleos',   # Total de n√∫cleos familiares\n    'tot_personas'   # Total de personas\n]\n\ncategorical_features = [\n    's3_vi01',       # Tipo de vivienda\n    's3_vi03',       # Material de piso\n    's3_vi04',       # Material de techo  \n    's3_vi05',       # Material de paredes\n    's4_ho01',       # Procedencia del agua\n    's4_ho06',       # Servicio el√©ctrico\n    's4_ho08',       # Servicio higi√©nico\n    's4_ho12',       # Tel√©fono convencional\n    's4_ho16',       # Eliminaci√≥n de basura\n    's4_ho17'        # Combustible para cocinar\n]\n\nall_features = numerical_features + categorical_features\n\n# Verificar qu√© columnas existen\navailable_numerical = [col for col in numerical_features if col in df.columns]\navailable_categorical = [col for col in categorical_features if col in df.columns]\navailable_features = available_numerical + available_categorical\n\nprint(f\"Features num√©ricas disponibles: {available_numerical}\")\nprint(f\"Features categ√≥ricas disponibles: {available_categorical}\")\nprint(f\"Total features: {len(available_features)}\")\n\n\n=== SELECCIONANDO Y CLASIFICANDO FEATURES ===\nColumnas disponibles en el dataset:\n['s1_id06_cod', 's1_id06_des', 's1_id05_cod', 's1_id05_des', 's1_id04_cod', 's1_id04_des', 's1_id03', 's3_vi02', 's3_vi05_est', 's3_vi04_est', 's3_vi03_est', 's3_vi05', 's3_vi04', 's3_vi03', 's3_vi01', 's4_ho08', 's4_ho17', 's4_ho16', 's4_ho06', 's4_ho01', 's4_ho19', 's4_ho12', 's4_ho21', 's4_ho22', 'decil', 'tipo_pob_rs18', 'tot_hogares', 'tot_nucleos', 'tot_personas']\nFeatures num√©ricas disponibles: ['tot_hogares', 'tot_nucleos', 'tot_personas']\nFeatures categ√≥ricas disponibles: ['s3_vi01', 's3_vi03', 's3_vi04', 's3_vi05', 's4_ho01', 's4_ho06', 's4_ho08', 's4_ho12', 's4_ho16', 's4_ho17']\nTotal features: 13"
  },
  {
    "objectID": "regresion_hogares.html#preparaci√≥n-de-datos",
    "href": "regresion_hogares.html#preparaci√≥n-de-datos",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== PREPARANDO DATOS PARA EL MODELO ===\")\n# Crear dataset limpio\nmodel_data = df[available_features + [target_col]].copy()\n\n# Eliminar filas con valores faltantes\ninitial_rows = len(model_data)\nmodel_data = model_data.dropna()\nfinal_rows = len(model_data)\n\nprint(f\"Filas iniciales: {initial_rows}\")\nprint(f\"Filas despu√©s de limpiar: {final_rows}\")\nprint(f\"Filas eliminadas: {initial_rows - final_rows}\")\n\n# Separar features y target\nX = model_data[available_features]\ny = model_data[target_col]\n\nprint(f\"Forma final de X: {X.shape}\")\nprint(f\"Forma final de y: {y.shape}\")\nprint(f\"Features num√©ricas: {available_numerical}\")\nprint(f\"Features categ√≥ricas: {available_categorical}\")\n\n\n=== PREPARANDO DATOS PARA EL MODELO ===\nFilas iniciales: 2565433\nFilas despu√©s de limpiar: 2565433\nFilas eliminadas: 0\nForma final de X: (2565433, 13)\nForma final de y: (2565433,)\nFeatures num√©ricas: ['tot_hogares', 'tot_nucleos', 'tot_personas']\nFeatures categ√≥ricas: ['s3_vi01', 's3_vi03', 's3_vi04', 's3_vi05', 's4_ho01', 's4_ho06', 's4_ho08', 's4_ho12', 's4_ho16', 's4_ho17']\n\n\n\n\nCode\n# Estad√≠sticas de las features\nprint(\"=== ESTAD√çSTICAS DE FEATURES ===\")\nif len(available_features) &gt; 0:\n    X.describe()\nelse:\n    print(\"No hay features disponibles para mostrar estad√≠sticas\")\n\n\n=== ESTAD√çSTICAS DE FEATURES ==="
  },
  {
    "objectID": "regresion_hogares.html#divisi√≥n-de-datos",
    "href": "regresion_hogares.html#divisi√≥n-de-datos",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== DIVIDIENDO DATOS ===\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Entrenamiento: {X_train.shape[0]} muestras\")\nprint(f\"Prueba: {X_test.shape[0]} muestras\")\nprint(f\"Estad√≠sticas del target en entrenamiento:\")\nprint(f\"Media: {y_train.mean():.2f}\")\nprint(f\"Desviaci√≥n est√°ndar: {y_train.std():.2f}\")\n\n\n=== DIVIDIENDO DATOS ===\nEntrenamiento: 2052346 muestras\nPrueba: 513087 muestras\nEstad√≠sticas del target en entrenamiento:\nMedia: 3.98\nDesviaci√≥n est√°ndar: 2.36"
  },
  {
    "objectID": "regresion_hogares.html#definici√≥n-y-entrenamiento-del-modelo",
    "href": "regresion_hogares.html#definici√≥n-y-entrenamiento-del-modelo",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== CREANDO PIPELINE DE REGRESI√ìN ===\")\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Crear pipeline de preprocesamiento correcto\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), available_numerical),\n        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), available_categorical)\n    ])\n\n# Pipeline completo con preprocesamiento adecuado\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])\n\n# Entrenar modelo\nprint(\"Entrenando modelo LinearRegression con preprocesamiento correcto...\")\nprint(f\"Variables num√©ricas: {available_numerical}\")\nprint(f\"Variables categ√≥ricas: {available_categorical}\")\npipeline.fit(X_train, y_train)\nprint(\"‚úì Modelo entrenado exitosamente\")\n\n\n=== CREANDO PIPELINE DE REGRESI√ìN ===\nEntrenando modelo LinearRegression con preprocesamiento correcto...\nVariables num√©ricas: ['tot_hogares', 'tot_nucleos', 'tot_personas']\nVariables categ√≥ricas: ['s3_vi01', 's3_vi03', 's3_vi04', 's3_vi05', 's4_ho01', 's4_ho06', 's4_ho08', 's4_ho12', 's4_ho16', 's4_ho17']\n‚úì Modelo entrenado exitosamente"
  },
  {
    "objectID": "regresion_hogares.html#generaci√≥n-de-predicciones",
    "href": "regresion_hogares.html#generaci√≥n-de-predicciones",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== REALIZANDO PREDICCIONES ===\")\ny_pred_train = pipeline.predict(X_train)\ny_pred_test = pipeline.predict(X_test)\n\nprint(f\"Predicciones generadas para {len(y_pred_test)} muestras de prueba\")\nprint(f\"Rango de predicciones: [{y_pred_test.min():.2f}, {y_pred_test.max():.2f}]\")\nprint(f\"Rango real: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n\n\n=== REALIZANDO PREDICCIONES ===\nPredicciones generadas para 513087 muestras de prueba\nRango de predicciones: [-45.74, 86.93]\nRango real: [1.00, 10.00]"
  },
  {
    "objectID": "regresion_hogares.html#evaluaci√≥n-del-modelo",
    "href": "regresion_hogares.html#evaluaci√≥n-del-modelo",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== EVALUACI√ìN DEL MODELO ===\")\n# M√©tricas en conjunto de entrenamiento\nmse_train = mean_squared_error(y_train, y_pred_train)\nrmse_train = np.sqrt(mse_train)\nr2_train = r2_score(y_train, y_pred_train)\n\n# M√©tricas en conjunto de prueba\nmse_test = mean_squared_error(y_test, y_pred_test)\nrmse_test = np.sqrt(mse_test)\nr2_test = r2_score(y_test, y_pred_test)\n\n# Crear DataFrame con m√©tricas\nmetricas = pd.DataFrame({\n    'M√©trica': ['MSE', 'RMSE', 'R¬≤ Score'],\n    'Entrenamiento': [mse_train, rmse_train, r2_train],\n    'Prueba': [mse_test, rmse_test, r2_test],\n    'Interpretaci√≥n': [\n        f'Error cuadr√°tico medio: {mse_test:.3f}',\n        f'Error promedio: ¬±{rmse_test:.2f} deciles',\n        f'Varianza explicada: {r2_test*100:.1f}%'\n    ]\n})\n\nprint(metricas.to_string(index=False))\n\n\n=== EVALUACI√ìN DEL MODELO ===\n M√©trica  Entrenamiento   Prueba                Interpretaci√≥n\n     MSE       3.466656 3.385502 Error cuadr√°tico medio: 3.386\n    RMSE       1.861896 1.839973 Error promedio: ¬±1.84 deciles\nR¬≤ Score       0.378183 0.392704     Varianza explicada: 39.3%"
  },
  {
    "objectID": "regresion_hogares.html#visualizaciones",
    "href": "regresion_hogares.html#visualizaciones",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nplt.figure(figsize=(10, 8))\nplt.scatter(y_test, y_pred_test, alpha=0.6, color='blue', s=20)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Deciles Reales')\nplt.ylabel('Deciles Predichos')\nplt.title('Valores Reales vs Predichos - Deciles Socioecon√≥micos', fontsize=14, fontweight='bold')\nplt.grid(alpha=0.3)\n\n# A√±adir m√©tricas al gr√°fico\nplt.text(0.05, 0.95, f'R¬≤ = {r2_test:.3f}\\nRMSE = {rmse_test:.2f}', \n         transform=plt.gca().transAxes, fontsize=12, \n         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(12, 5))\n\n# Subplot 1: Histograma de errores\nplt.subplot(1, 2, 1)\nerrors = y_test - y_pred_test\nplt.hist(errors, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')\nplt.xlabel('Error (Real - Predicho)')\nplt.ylabel('Frecuencia')\nplt.title('Distribuci√≥n de Errores', fontweight='bold')\nplt.axvline(x=0, color='red', linestyle='--', alpha=0.8)\nplt.grid(alpha=0.3)\n\n# Subplot 2: Errores vs valores predichos\nplt.subplot(1, 2, 2)\nplt.scatter(y_pred_test, errors, alpha=0.6, color='green', s=20)\nplt.xlabel('Valores Predichos')\nplt.ylabel('Error (Real - Predicho)')\nplt.title('Errores vs Valores Predichos', fontweight='bold')\nplt.axhline(y=0, color='red', linestyle='--', alpha=0.8)\nplt.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"=== GENERANDO CURVA DE APRENDIZAJE ===\")\ntrain_sizes, train_scores, val_scores = learning_curve(\n    pipeline, X_train, y_train, cv=5, \n    train_sizes=np.linspace(0.1, 1.0, 10),\n    scoring='r2', random_state=42\n)\n\n# Calcular medias y desviaciones est√°ndar\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(train_sizes, train_mean, 'o-', color='blue', label='Entrenamiento')\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\nplt.plot(train_sizes, val_mean, 'o-', color='red', label='Validaci√≥n')\nplt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n\nplt.xlabel('Tama√±o del Conjunto de Entrenamiento')\nplt.ylabel('R¬≤ Score')\nplt.title('Curva de Aprendizaje - Regresi√≥n de Deciles', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n=== GENERANDO CURVA DE APRENDIZAJE ==="
  },
  {
    "objectID": "regresion_hogares.html#an√°lisis-de-importancia-de-features",
    "href": "regresion_hogares.html#an√°lisis-de-importancia-de-features",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== AN√ÅLISIS DE IMPORTANCIA DE FEATURES ===\")\n# Obtener nombres de features despu√©s del preprocesamiento\nfeature_names = (available_numerical + \n                list(pipeline.named_steps['preprocessor']\n                    .named_transformers_['cat']\n                    .get_feature_names_out(available_categorical)))\n\ncoefficients = pipeline.named_steps['regressor'].coef_\n\n# Crear DataFrame con importancias\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'coefficient': coefficients,\n    'abs_coefficient': np.abs(coefficients)\n}).sort_values('abs_coefficient', ascending=False)\n\n# Mapeo de nombres descriptivos para interpretaci√≥n\nfeature_descriptions = {\n    'tot_personas': 'Total de personas en el hogar',\n    'tot_nucleos': 'Total de n√∫cleos familiares', \n    'tot_hogares': 'Total de hogares en el √°rea'\n}\n\nprint(\"Top 15 features m√°s importantes:\")\nfor _, row in feature_importance.head(15).iterrows():\n    var_name = row['feature']\n    coef = row['coefficient']\n    \n    # Descripci√≥n m√°s clara para variables categ√≥ricas\n    if var_name in feature_descriptions:\n        description = feature_descriptions[var_name]\n    elif 's3_vi03' in var_name:\n        description = f'Material del piso (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's3_vi04' in var_name:\n        description = f'Material del techo (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's3_vi05' in var_name:\n        description = f'Material de paredes (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho01' in var_name:\n        description = f'Procedencia del agua (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho06' in var_name:\n        description = f'Servicio el√©ctrico (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho08' in var_name:\n        description = f'Servicio higi√©nico (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho12' in var_name:\n        description = f'Tel√©fono fijo (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho16' in var_name:\n        description = f'Recolecci√≥n de basura (categor√≠a {var_name.split(\"_\")[-1]})'\n    elif 's4_ho17' in var_name:\n        description = f'Combustible para cocinar (categor√≠a {var_name.split(\"_\")[-1]})'\n    else:\n        description = 'Variable del hogar'\n    \n    print(f\"{var_name:25} | {coef:8.3f} | {description}\")\n\n\n=== AN√ÅLISIS DE IMPORTANCIA DE FEATURES ===\nTop 15 features m√°s importantes:\ntot_personas              |   -2.653 | Total de personas en el hogar\ns3_vi04_7                 |   -1.847 | Material del techo (categor√≠a 7)\ntot_nucleos               |    1.704 | Total de n√∫cleos familiares\ns3_vi04_6                 |   -1.578 | Material del techo (categor√≠a 6)\ns3_vi04_4                 |   -1.436 | Material del techo (categor√≠a 4)\ns3_vi04_5                 |   -1.260 | Material del techo (categor√≠a 5)\ns3_vi04_8                 |   -1.080 | Material del techo (categor√≠a 8)\ns4_ho12_5                 |   -0.965 | Tel√©fono fijo (categor√≠a 5)\ns4_ho12_6                 |   -0.955 | Tel√©fono fijo (categor√≠a 6)\ns4_ho12_3                 |   -0.897 | Tel√©fono fijo (categor√≠a 3)\ntot_hogares               |    0.859 | Total de hogares en el √°rea\ns4_ho17_3                 |    0.799 | Combustible para cocinar (categor√≠a 3)\ns4_ho01_3                 |   -0.794 | Procedencia del agua (categor√≠a 3)\ns4_ho17_2                 |   -0.750 | Combustible para cocinar (categor√≠a 2)\ns4_ho01_4                 |   -0.713 | Procedencia del agua (categor√≠a 4)\n\n\n\n\nCode\n# Visualizar importancia de features (solo top 15)\nplt.figure(figsize=(12, 8))\ntop_features = feature_importance.head(15)  # Solo las 15 m√°s importantes\ncolors = ['red' if coef &lt; 0 else 'green' for coef in top_features['coefficient']]\nplt.barh(range(len(top_features)), top_features['coefficient'], color=colors)\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Coeficiente')\nplt.title('Top 15 Features M√°s Importantes (Coeficientes de Regresi√≥n)', fontsize=14, fontweight='bold')\nplt.grid(axis='x', alpha=0.3)\n\n# A√±adir l√≠nea en x=0\nplt.axvline(x=0, color='black', linestyle='-', alpha=0.8)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "regresion_hogares.html#an√°lisis-por-deciles",
    "href": "regresion_hogares.html#an√°lisis-por-deciles",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Code\nprint(\"=== AN√ÅLISIS POR DECILES ===\")\n# Crear DataFrame con resultados por decil\nresults_by_decil = pd.DataFrame({\n    'decil_real': y_test,\n    'decil_predicho': y_pred_test,\n    'error': y_test - y_pred_test,\n    'error_abs': np.abs(y_test - y_pred_test)\n})\n\n# Estad√≠sticas por decil real\ndecil_stats = results_by_decil.groupby('decil_real').agg({\n    'decil_predicho': ['mean', 'std'],\n    'error': ['mean', 'std'],\n    'error_abs': 'mean'\n}).round(3)\n\nprint(\"Estad√≠sticas por decil real:\")\nprint(decil_stats)\n\n\n=== AN√ÅLISIS POR DECILES ===\nEstad√≠sticas por decil real:\n           decil_predicho         error        error_abs\n                     mean    std   mean    std      mean\ndecil_real                                              \n1                   2.513  1.048 -1.513  1.048     1.569\n2                   3.299  1.036 -1.299  1.036     1.379\n3                   3.798  1.047 -0.798  1.047     1.038\n4                   4.171  1.061 -0.171  1.061     0.841\n5                   4.461  1.061  0.539  1.061     0.958\n6                   4.724  1.095  1.276  1.095     1.381\n7                   4.989  1.117  2.011  1.117     2.045\n8                   5.334  1.338  2.666  1.338     2.721\n9                   5.713  1.403  3.287  1.403     3.357\n10                  6.107  1.227  3.893  1.227     3.945\n\n\n\n\nCode\n# Visualizaci√≥n por deciles\nplt.figure(figsize=(12, 8))\n\n# Boxplot de predicciones por decil real\nplt.subplot(2, 2, 1)\nresults_by_decil.boxplot(column='decil_predicho', by='decil_real', ax=plt.gca())\nplt.title('Predicciones por Decil Real')\nplt.xlabel('Decil Real')\nplt.ylabel('Decil Predicho')\n\n# Boxplot de errores por decil real\nplt.subplot(2, 2, 2)\nresults_by_decil.boxplot(column='error_abs', by='decil_real', ax=plt.gca())\nplt.title('Error Absoluto por Decil Real')\nplt.xlabel('Decil Real')\nplt.ylabel('Error Absoluto')\n\n# Scatter plot con colores por decil\nplt.subplot(2, 2, 3)\nscatter = plt.scatter(results_by_decil['decil_real'], results_by_decil['decil_predicho'], \n                     c=results_by_decil['decil_real'], cmap='viridis', alpha=0.6)\nplt.plot([1, 10], [1, 10], 'r--', alpha=0.8)\nplt.xlabel('Decil Real')\nplt.ylabel('Decil Predicho')\nplt.title('Predicciones Coloreadas por Decil')\nplt.colorbar(scatter)\n\n# Histograma de errores\nplt.subplot(2, 2, 4)\nplt.hist(results_by_decil['error'], bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel('Error (Real - Predicho)')\nplt.ylabel('Frecuencia')\nplt.title('Distribuci√≥n de Errores')\nplt.axvline(x=0, color='red', linestyle='--', alpha=0.8)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "regresion_hogares.html#interpretaci√≥n-de-resultados",
    "href": "regresion_hogares.html#interpretaci√≥n-de-resultados",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "El modelo de regresi√≥n lineal con preprocesamiento correcto muestra los siguientes resultados:\n\nR¬≤ Score (0.393): El modelo explica el 39.3% de la varianza en los deciles socioecon√≥micos\nRMSE (1.84): Error promedio de aproximadamente 1.8 deciles\n\nMejora significativa: El tratamiento correcto de variables categ√≥ricas mejor√≥ el rendimiento predictivo\n\n\n\n\nEl an√°lisis con preprocesamiento correcto revela patrones m√°s precisos:\n\nComposici√≥n del hogar:\n\nTotal de personas (coef: -2.65): Hogares m√°s grandes tienden a ser m√°s pobres\nTotal de n√∫cleos (coef: +1.70): M√°s n√∫cleos familiares indica mejor situaci√≥n econ√≥mica\n\nMaterial del techo (altamente predictivo):\n\nCategor√≠as espec√≠ficas (4, 5, 6, 7, 8) est√°n fuertemente asociadas con pobreza\nEl material del techo es el segundo factor m√°s importante despu√©s del tama√±o del hogar\n\nAcceso a servicios de comunicaci√≥n:\n\nTel√©fono fijo: Ciertas categor√≠as (ausencia o tipo b√°sico) predicen deciles m√°s bajos\nRefleja el acceso a servicios modernos de comunicaci√≥n\n\nInfraestructura de la vivienda:\n\nMateriales de construcci√≥n espec√≠ficos son marcadores claros de nivel socioecon√≥mico\n\n\n\n\n\n\nDeciles bajos (1-3): Mejor predicci√≥n, caracter√≠sticas m√°s homog√©neas\nDeciles medios (4-7): Mayor variabilidad en las predicciones\nDeciles altos (8-10): Tendencia a subestimar el nivel socioecon√≥mico\n\n\n\n\nEl modelo presenta algunas limitaciones importantes que deben considerarse al interpretar los resultados:\n\nVarianza no explicada: El modelo explica aproximadamente el 39.3% de la variabilidad en los deciles socioecon√≥micos, lo que significa que existe un 60.7% de varianza que no est√° siendo capturada. Esto sugiere que hay factores socioecon√≥micos importantes que no est√°n incluidos en nuestro conjunto de variables, como ingresos familiares, educaci√≥n, empleo, etc.\nSimplicidad del modelo lineal: La regresi√≥n lineal asume relaciones lineales entre las variables, pero en la realidad socioecon√≥mica, las relaciones suelen ser m√°s complejas y no lineales.\nVariables seleccionadas limitadas: De las 29 caracter√≠sticas disponibles en el dataset, solo utilizamos 13 variables. Es posible que otras caracter√≠sticas como ubicaci√≥n geogr√°fica (provincia, cant√≥n) o variables adicionales de la vivienda podr√≠an mejorar la precisi√≥n del modelo."
  },
  {
    "objectID": "regresion_hogares.html#conclusiones",
    "href": "regresion_hogares.html#conclusiones",
    "title": "Regresi√≥n - Predicci√≥n de Deciles Socioecon√≥micos",
    "section": "",
    "text": "Material del techo es altamente predictivo: Las categor√≠as espec√≠ficas de materiales de construcci√≥n son marcadores claros de nivel socioecon√≥mico, siendo el segundo factor m√°s importante\nComposici√≥n del hogar es el factor m√°s cr√≠tico: Hogares con m√°s personas tienden a ser m√°s pobres, mientras que m√°s n√∫cleos familiares indica mejor situaci√≥n econ√≥mica\nServicios de comunicaci√≥n reflejan desigualdad: El acceso a tel√©fono fijo y su tipo son indicadores importantes del nivel socioecon√≥mico\nAplicaciones en pol√≠tica social: El modelo puede identificar hogares vulnerables bas√°ndose en caracter√≠sticas observables de la vivienda y composici√≥n familiar\n\n\n\n\nIdentificaci√≥n de vulnerabilidad: El modelo puede ayudar a identificar hogares en situaci√≥n de pobreza\nPlanificaci√≥n urbana: Priorizaci√≥n de inversiones en infraestructura"
  },
  {
    "objectID": "trabajo.html",
    "href": "trabajo.html",
    "title": "An√°lisis de contrataciones",
    "section": "",
    "text": "La siguiente secci√≥n muestra la distribuci√≥n de personas contratadas por cant√≥n para las provincias con mayor y menor n√∫mero total de contrataciones.\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDATA_PATH = \"lab 1 data/mdt_colocados_2025agosto.csv\"\n\ndf = pd.read_csv(DATA_PATH, sep=\";\", encoding=\"latin1\")\ndf = df.rename(\n    columns={\n        \"A¬§o\": \"A√±o\",\n        \"Cant¬¢n\": \"Cant√≥n\",\n        \"N¬£mero de Personas\": \"N√∫mero de Personas\",\n    }\n)\n\nprovincia_totales = df.groupby(\"Provincia\")[\"N√∫mero de Personas\"].sum().sort_values(ascending=False)\ntop4 = provincia_totales.head(4).index.tolist()\n\ndef plot_provincias(provincias, titulo, ranking_label):\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharey=True)\n    fig.suptitle(titulo, fontsize=14, weight=\"bold\")\n\n    for ax, provincia in zip(axes.flatten(), provincias):\n        serie = (\n            df[df[\"Provincia\"] == provincia]\n            .groupby(\"Cant√≥n\")[\"N√∫mero de Personas\"]\n            .sum()\n            .sort_values(ascending=False)\n        )\n        ax.bar(serie.index, serie.values, color=\"#4C72B0\")\n        ax.set_title(provincia, fontsize=10)\n        ax.tick_params(axis=\"x\", rotation=70, labelsize=8)\n        if ax in (axes[0, 0], axes[1, 0]):\n            ax.set_ylabel(\"Personas\")\n        else:\n            ax.set_ylabel(\"\")\n        ax.set_xlabel(\"Cant√≥n\", fontsize=9)\n\n        total_provincia = provincia_totales.loc[provincia]\n        ax.text(\n            0.98,\n            0.95,\n            f\"Total: {total_provincia:,}\",\n            transform=ax.transAxes,\n            fontsize=8,\n            fontweight=\"bold\",\n            ha=\"right\",\n            va=\"top\",\n        )\n\n        posicion = provincias.index(provincia) + 1\n        ax.text(\n            0.98,\n            0.85,\n            f\"{ranking_label} {posicion}\",\n            transform=ax.transAxes,\n            fontsize=8,\n            ha=\"right\",\n            va=\"top\",\n        )\n\n        for patch in ax.patches:\n            altura = patch.get_height()\n            if altura &gt; 0:\n                ax.text(\n                    patch.get_x() + patch.get_width() / 2,\n                    altura,\n                    f\"{int(altura):,}\",\n                    ha=\"center\",\n                    va=\"bottom\",\n                    fontsize=7,\n                )\n\n    for ax in axes.flatten()[len(provincias):]:\n        ax.axis(\"off\")\n\n    fig.tight_layout(rect=[0, 0, 1, 0.95])\n    return fig\n\nplot_provincias(top4, \"Provincias con m√°s contrataciones\", \"Top\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nbottom4 = provincia_totales.tail(4).sort_values().index.tolist()\nplot_provincias(bottom4, \"Provincias con menos contrataciones\", \"Bottom\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncuatrimestral = (\n    df.assign(Cuatrimestre=((df[\"Mes\"] - 1) // 4) + 1)\n    .groupby([\"A√±o\", \"Cuatrimestre\"])[\"N√∫mero de Personas\"]\n    .sum()\n    .reset_index()\n    .sort_values([\"A√±o\", \"Cuatrimestre\"])\n)\ncuatrimestral[\"Periodo\"] = (\n    cuatrimestral[\"A√±o\"].astype(str)\n    + \" C\"\n    + cuatrimestral[\"Cuatrimestre\"].astype(str)\n)\n\nplt.figure(figsize=(10, 5))\nplt.plot(\n    cuatrimestral[\"Periodo\"],\n    cuatrimestral[\"N√∫mero de Personas\"],\n    marker=\"o\",\n    color=\"#4C72B0\",\n)\nplt.title(\"Evoluci√≥n cuatrimestral de contrataciones\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Periodo cuatrimestral\")\nplt.ylabel(\"Personas contratadas\")\nplt.xticks(rotation=45)\n\nfor etiqueta, valor in zip(\n    cuatrimestral[\"Periodo\"], cuatrimestral[\"N√∫mero de Personas\"]\n):\n    plt.text(\n        etiqueta,\n        valor,\n        f\"{int(valor):,}\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=8,\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDATA_PATH = \"lab 1 data/mdt_colocados_2025agosto.csv\"\n\n# Cargar y preparar los datos\ndf = pd.read_csv(DATA_PATH, sep=\";\", encoding=\"latin1\").rename(\n    columns={\n        \"A¬§o\": \"A√±o\",\n        \"Cant¬¢n\": \"Cant√≥n\",\n        \"N¬£mero de Personas\": \"N√∫mero de Personas\",\n    }\n)\n\nprovincia_totales = df.groupby(\"Provincia\")[\"N√∫mero de Personas\"].sum().sort_values(ascending=False)\n\nTOP_N = 12\npie_datos = provincia_totales.reset_index().rename(columns={\"N√∫mero de Personas\": \"Personas\"})\ntop_provincias = pie_datos.head(TOP_N)\nresto_total = pie_datos[\"Personas\"][TOP_N:].sum()\n\nif resto_total &gt; 0:\n    pie_data = pd.concat(\n        [top_provincias, pd.DataFrame({\"Provincia\": [\"Otros\"], \"Personas\": [resto_total]})],\n        ignore_index=True,\n    )\nelse:\n    pie_data = top_provincias\n\n\ndef _autopct(values):\n    total = values.sum()\n\n    def inner(pct):\n        valor = int(round(total * pct / 100.0))\n        return f\"{pct:.1f}%\\n{valor:,}\"\n\n    return inner\n\n\nplt.figure(figsize=(8, 8))\nplt.pie(\n    pie_data[\"Personas\"],\n    labels=pie_data[\"Provincia\"],\n    autopct=_autopct(pie_data[\"Personas\"]),\n    startangle=90,\n    pctdistance=0.78,\n    textprops={\"fontsize\": 9},\n)\nplt.title(\"Participaci√≥n de contrataciones por provincia\", fontsize=14, weight=\"bold\")\ncentre_circle = plt.Circle((0, 0), 0.60, fc=\"white\")\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "trabajo.html#provincias-m√°s-contratadas",
    "href": "trabajo.html#provincias-m√°s-contratadas",
    "title": "An√°lisis de contrataciones",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDATA_PATH = \"lab 1 data/mdt_colocados_2025agosto.csv\"\n\ndf = pd.read_csv(DATA_PATH, sep=\";\", encoding=\"latin1\")\ndf = df.rename(\n    columns={\n        \"A¬§o\": \"A√±o\",\n        \"Cant¬¢n\": \"Cant√≥n\",\n        \"N¬£mero de Personas\": \"N√∫mero de Personas\",\n    }\n)\n\nprovincia_totales = df.groupby(\"Provincia\")[\"N√∫mero de Personas\"].sum().sort_values(ascending=False)\ntop4 = provincia_totales.head(4).index.tolist()\n\ndef plot_provincias(provincias, titulo, ranking_label):\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharey=True)\n    fig.suptitle(titulo, fontsize=14, weight=\"bold\")\n\n    for ax, provincia in zip(axes.flatten(), provincias):\n        serie = (\n            df[df[\"Provincia\"] == provincia]\n            .groupby(\"Cant√≥n\")[\"N√∫mero de Personas\"]\n            .sum()\n            .sort_values(ascending=False)\n        )\n        ax.bar(serie.index, serie.values, color=\"#4C72B0\")\n        ax.set_title(provincia, fontsize=10)\n        ax.tick_params(axis=\"x\", rotation=70, labelsize=8)\n        if ax in (axes[0, 0], axes[1, 0]):\n            ax.set_ylabel(\"Personas\")\n        else:\n            ax.set_ylabel(\"\")\n        ax.set_xlabel(\"Cant√≥n\", fontsize=9)\n\n        total_provincia = provincia_totales.loc[provincia]\n        ax.text(\n            0.98,\n            0.95,\n            f\"Total: {total_provincia:,}\",\n            transform=ax.transAxes,\n            fontsize=8,\n            fontweight=\"bold\",\n            ha=\"right\",\n            va=\"top\",\n        )\n\n        posicion = provincias.index(provincia) + 1\n        ax.text(\n            0.98,\n            0.85,\n            f\"{ranking_label} {posicion}\",\n            transform=ax.transAxes,\n            fontsize=8,\n            ha=\"right\",\n            va=\"top\",\n        )\n\n        for patch in ax.patches:\n            altura = patch.get_height()\n            if altura &gt; 0:\n                ax.text(\n                    patch.get_x() + patch.get_width() / 2,\n                    altura,\n                    f\"{int(altura):,}\",\n                    ha=\"center\",\n                    va=\"bottom\",\n                    fontsize=7,\n                )\n\n    for ax in axes.flatten()[len(provincias):]:\n        ax.axis(\"off\")\n\n    fig.tight_layout(rect=[0, 0, 1, 0.95])\n    return fig\n\nplot_provincias(top4, \"Provincias con m√°s contrataciones\", \"Top\")\nplt.show()"
  },
  {
    "objectID": "trabajo.html#provincias-menos-contratadas",
    "href": "trabajo.html#provincias-menos-contratadas",
    "title": "An√°lisis de contrataciones",
    "section": "",
    "text": "Code\nbottom4 = provincia_totales.tail(4).sort_values().index.tolist()\nplot_provincias(bottom4, \"Provincias con menos contrataciones\", \"Bottom\")\nplt.show()"
  },
  {
    "objectID": "trabajo.html#evoluci√≥n-cuatrimestral-de-contrataciones",
    "href": "trabajo.html#evoluci√≥n-cuatrimestral-de-contrataciones",
    "title": "An√°lisis de contrataciones",
    "section": "",
    "text": "Code\ncuatrimestral = (\n    df.assign(Cuatrimestre=((df[\"Mes\"] - 1) // 4) + 1)\n    .groupby([\"A√±o\", \"Cuatrimestre\"])[\"N√∫mero de Personas\"]\n    .sum()\n    .reset_index()\n    .sort_values([\"A√±o\", \"Cuatrimestre\"])\n)\ncuatrimestral[\"Periodo\"] = (\n    cuatrimestral[\"A√±o\"].astype(str)\n    + \" C\"\n    + cuatrimestral[\"Cuatrimestre\"].astype(str)\n)\n\nplt.figure(figsize=(10, 5))\nplt.plot(\n    cuatrimestral[\"Periodo\"],\n    cuatrimestral[\"N√∫mero de Personas\"],\n    marker=\"o\",\n    color=\"#4C72B0\",\n)\nplt.title(\"Evoluci√≥n cuatrimestral de contrataciones\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Periodo cuatrimestral\")\nplt.ylabel(\"Personas contratadas\")\nplt.xticks(rotation=45)\n\nfor etiqueta, valor in zip(\n    cuatrimestral[\"Periodo\"], cuatrimestral[\"N√∫mero de Personas\"]\n):\n    plt.text(\n        etiqueta,\n        valor,\n        f\"{int(valor):,}\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=8,\n    )\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "trabajo.html#trabajo-por-provincia",
    "href": "trabajo.html#trabajo-por-provincia",
    "title": "An√°lisis de contrataciones",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDATA_PATH = \"lab 1 data/mdt_colocados_2025agosto.csv\"\n\n# Cargar y preparar los datos\ndf = pd.read_csv(DATA_PATH, sep=\";\", encoding=\"latin1\").rename(\n    columns={\n        \"A¬§o\": \"A√±o\",\n        \"Cant¬¢n\": \"Cant√≥n\",\n        \"N¬£mero de Personas\": \"N√∫mero de Personas\",\n    }\n)\n\nprovincia_totales = df.groupby(\"Provincia\")[\"N√∫mero de Personas\"].sum().sort_values(ascending=False)\n\nTOP_N = 12\npie_datos = provincia_totales.reset_index().rename(columns={\"N√∫mero de Personas\": \"Personas\"})\ntop_provincias = pie_datos.head(TOP_N)\nresto_total = pie_datos[\"Personas\"][TOP_N:].sum()\n\nif resto_total &gt; 0:\n    pie_data = pd.concat(\n        [top_provincias, pd.DataFrame({\"Provincia\": [\"Otros\"], \"Personas\": [resto_total]})],\n        ignore_index=True,\n    )\nelse:\n    pie_data = top_provincias\n\n\ndef _autopct(values):\n    total = values.sum()\n\n    def inner(pct):\n        valor = int(round(total * pct / 100.0))\n        return f\"{pct:.1f}%\\n{valor:,}\"\n\n    return inner\n\n\nplt.figure(figsize=(8, 8))\nplt.pie(\n    pie_data[\"Personas\"],\n    labels=pie_data[\"Provincia\"],\n    autopct=_autopct(pie_data[\"Personas\"]),\n    startangle=90,\n    pctdistance=0.78,\n    textprops={\"fontsize\": 9},\n)\nplt.title(\"Participaci√≥n de contrataciones por provincia\", fontsize=14, weight=\"bold\")\ncentre_circle = plt.Circle((0, 0), 0.60, fc=\"white\")\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Primer proyecto en Quarto",
    "section": "",
    "text": "T√≠tulo -\n\n\nSubt√≠tulo - second\n\n\nCode\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x='Miles_per_Gallon',\n    y='Horsepower',\n    tooltip=['Name', 'Miles_per_Gallon', 'Horsepower', 'Origin'],\n    color='Origin'\n).interactive()\n\n\n\n\n\n\n\n\nVisualisacion scatter plot with shape and color\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x='Miles_per_Gallon',\n    y='Horsepower',\n    tooltip=['Name', 'Miles_per_Gallon', 'Horsepower', 'Origin'],\n    color='Origin',\n    shape='Origin'\n).interactive()\n\n\n\n\n\n\n\n\n\nAgregaciones\nGrafica de barras\nNumber de observaciones\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin', title='Origen', sort='-y'),\n    alt.Y('count()', title='N√∫mero de observaciones')\n).interactive()\n\n\n\n\n\n\n\n‚ÄòMean()‚Äô de cada uno de los orgienes\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin', title='Origen', sort='-y'),\n    alt.Y('mean(Weight_in_lbs)', title='Promedio de Weight_in_lbs'),\n    tooltip=['Origin', 'mean(Weight_in_lbs)']\n).properties(title='Promedio de Weight_in_lbs por Origen', width=200, height=200)\n\n\n\n\n\n\n\nGrafico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year', title='A√±o'),\n    alt.Y('mean(Weight_in_lbs)', title='Promedio de Weight_in_lbs'),\n    alt.Color('Origin')\n).properties(title='Promedio de Weight_in_lbs por A√±o', width=600).interactive()"
  },
  {
    "objectID": "clasificacion_binaria.html",
    "href": "clasificacion_binaria.html",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Este documento presenta un modelo de clasificaci√≥n binaria para predecir si un siniestro de tr√°nsito ser√° letal (con fallecidos) o no letal, utilizando datos del INEC Ecuador 2019.\nObjetivo: Desarrollar un sistema predictivo que identifique factores de riesgo asociados con la letalidad de accidentes de tr√°nsito.\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                           f1_score, confusion_matrix, classification_report, \n                           roc_curve, auc, roc_auc_score)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurar matplotlib\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"=== SISTEMA DE CLASIFICACI√ìN BINARIA - SINIESTROS LETALES ===\")\n\n\n=== SISTEMA DE CLASIFICACI√ìN BINARIA - SINIESTROS LETALES ===\n\n\n\n\nCode\n# Cargar datos\ndf = pd.read_csv('DATOS_CLASIFICACION_BINARIA/inec_anuario-de-estadisticas-de-transporte_siniestros-de-transito_2019.csv', \n                 sep=';', encoding='utf-8')\n\nprint(f\"Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\nprint(\"\\nPrimeras 5 filas:\")\ndf.head()\n\n\nDatos cargados: 24595 filas, 11 columnas\n\nPrimeras 5 filas:\n\n\n\n\n\n\n\n\n\nMES\nDIA\nHORA\nPROVINCIA\nCANTON\nZONA\nNUM_FALLECIDO\nNUM_LESIONADO\nTOTAL_VICTIMAS\nCLASE\nCAUSA\n\n\n\n\n0\nENERO\nMARTES\n11:00 A 11:59\nIMBABURA\nOTAVALO\nURBANA\n0\n0\n0\nP√âRDIDA DE PISTA\nIMPERICIA E IMPRUDENCIA DEL CONDUCTOR\n\n\n1\nENERO\nMARTES\n23:00 A 23:59\nIMBABURA\nIBARRA\nRURAL\n0\n0\n0\nOTROS\nOTRAS CAUSAS\n\n\n2\nENERO\nMARTES\n12:00 A 12:59\nTUNGURAHUA\nAMBATO\nURBANA\n0\n3\n3\nCHOQUES\nNO RESPETA LAS SE√ëALES DE TR√ÅNSITO\n\n\n3\nENERO\nMARTES\n07:00 A 07:59\nTUNGURAHUA\nAMBATO\nRURAL\n0\n1\n1\nATROPELLOS\nEMBRIAGUEZ O DROGA\n\n\n4\nENERO\nMARTES\n04:00 A 04:59\nPICHINCHA\nDISTRITO METROPOLITANO DE QUITO\nURBANA\n0\n0\n0\nESTRELLAMIENTOS\nEXCESO VELOCIDAD\n\n\n\n\n\n\n\n\n\nCode\n# Exploraci√≥n inicial\nprint(\"=== EXPLORACI√ìN DE DATOS ===\")\nprint(f\"Columnas: {list(df.columns)}\")\nprint(f\"\\nValores faltantes por columna:\")\ndf.isnull().sum()\n\n\n=== EXPLORACI√ìN DE DATOS ===\nColumnas: ['MES', 'DIA', 'HORA', 'PROVINCIA', 'CANTON', 'ZONA', 'NUM_FALLECIDO', 'NUM_LESIONADO', 'TOTAL_VICTIMAS', 'CLASE', 'CAUSA']\n\nValores faltantes por columna:\n\n\nMES               0\nDIA               0\nHORA              0\nPROVINCIA         0\nCANTON            0\nZONA              0\nNUM_FALLECIDO     0\nNUM_LESIONADO     0\nTOTAL_VICTIMAS    0\nCLASE             0\nCAUSA             0\ndtype: int64\n\n\n\n\n\n\n\nCode\n# Crear variable objetivo binaria\nprint(\"=== CREANDO VARIABLE OBJETIVO ===\")\ndf['SINIESTRO_LETAL'] = (df['NUM_FALLECIDO'] &gt; 0).astype(int)\n\nprint(f\"Distribuci√≥n de la variable objetivo:\")\nprint(df['SINIESTRO_LETAL'].value_counts())\nprint(f\"Porcentaje de siniestros letales: {df['SINIESTRO_LETAL'].mean()*100:.2f}%\")\n\n# Visualizar distribuci√≥n\nplt.figure(figsize=(8, 5))\ndf['SINIESTRO_LETAL'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\nplt.title('Distribuci√≥n de Siniestros: Letales vs No Letales')\nplt.xlabel('Tipo de Siniestro')\nplt.ylabel('Cantidad')\nplt.xticks([0, 1], ['No Letal', 'Letal'], rotation=0)\nplt.show()\n\n\n=== CREANDO VARIABLE OBJETIVO ===\nDistribuci√≥n de la variable objetivo:\n0    22626\n1     1969\nName: SINIESTRO_LETAL, dtype: int64\nPorcentaje de siniestros letales: 8.01%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(\"=== PREPARANDO FEATURES ===\")\n# Seleccionar columnas para el modelo\nfeature_cols = ['MES', 'DIA', 'HORA', 'PROVINCIA', 'CANTON', 'ZONA', 'CLASE', 'CAUSA']\nX = df[feature_cols].copy()\ny = df['SINIESTRO_LETAL']\n\nprint(f\"Features seleccionadas: {feature_cols}\")\nprint(f\"Forma de X: {X.shape}\")\n\n# Procesar la columna HORA para extraer la hora num√©rica\ndef extract_hour(hora_str):\n    try:\n        return int(hora_str.split(':')[0])\n    except:\n        return 12  # valor por defecto\n\nX['HORA_NUM'] = X['HORA'].apply(extract_hour)\nX = X.drop('HORA', axis=1)\n\n# Identificar variables categ√≥ricas y num√©ricas\ncategorical_features = ['MES', 'DIA', 'PROVINCIA', 'CANTON', 'ZONA', 'CLASE', 'CAUSA']\nnumerical_features = ['HORA_NUM']\n\nprint(f\"Features categ√≥ricas: {categorical_features}\")\nprint(f\"Features num√©ricas: {numerical_features}\")\n\n\n=== PREPARANDO FEATURES ===\nFeatures seleccionadas: ['MES', 'DIA', 'HORA', 'PROVINCIA', 'CANTON', 'ZONA', 'CLASE', 'CAUSA']\nForma de X: (24595, 8)\nFeatures categ√≥ricas: ['MES', 'DIA', 'PROVINCIA', 'CANTON', 'ZONA', 'CLASE', 'CAUSA']\nFeatures num√©ricas: ['HORA_NUM']\n\n\n\n\n\n\n\nCode\nprint(\"=== DIVIDIENDO DATOS ===\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Entrenamiento: {X_train.shape[0]} muestras\")\nprint(f\"Prueba: {X_test.shape[0]} muestras\")\nprint(f\"Distribuci√≥n en entrenamiento:\")\nprint(y_train.value_counts(normalize=True))\n\n\n=== DIVIDIENDO DATOS ===\nEntrenamiento: 19676 muestras\nPrueba: 4919 muestras\nDistribuci√≥n en entrenamiento:\n0    0.919953\n1    0.080047\nName: SINIESTRO_LETAL, dtype: float64\n\n\n\n\n\n\n\nCode\nprint(\"=== CREANDO PIPELINE ===\")\n# Crear pipeline de preprocesamiento\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n    ])\n\n# Pipeline completo\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n])\n\n# Entrenar modelo\nprint(\"Entrenando modelo LogisticRegression...\")\npipeline.fit(X_train, y_train)\nprint(\"‚úì Modelo entrenado exitosamente\")\n\n\n=== CREANDO PIPELINE ===\nEntrenando modelo LogisticRegression...\n‚úì Modelo entrenado exitosamente\n\n\n\n\n\n\n\nCode\nprint(\"=== REALIZANDO PREDICCIONES ===\")\ny_pred = pipeline.predict(X_test)\ny_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n\nprint(f\"Predicciones generadas para {len(y_pred)} muestras\")\nprint(f\"Distribuci√≥n de predicciones:\")\nprint(pd.Series(y_pred).value_counts())\n\n\n=== REALIZANDO PREDICCIONES ===\nPredicciones generadas para 4919 muestras\nDistribuci√≥n de predicciones:\n0    4860\n1      59\ndtype: int64\n\n\n\n\n\n\n\nCode\nprint(\"=== EVALUACI√ìN DEL MODELO ===\")\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Crear DataFrame con m√©tricas\nmetricas = pd.DataFrame({\n    'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n    'Valor': [accuracy, precision, recall, f1, roc_auc],\n    'Interpretaci√≥n': [\n        f'Clasifica correctamente el {accuracy*100:.1f}% de los casos',\n        f'De los predichos como letales, {precision*100:.1f}% son realmente letales',\n        f'Detecta el {recall*100:.1f}% de los siniestros letales reales',\n        f'Balance entre precision y recall: {f1:.3f}',\n        f'Capacidad de discriminaci√≥n: {roc_auc:.3f}'\n    ]\n})\n\nprint(metricas.to_string(index=False))\n\n\n=== EVALUACI√ìN DEL MODELO ===\n  M√©trica    Valor                                             Interpretaci√≥n\n Accuracy 0.923765              Clasifica correctamente el 92.4% de los casos\nPrecision 0.661017 De los predichos como letales, 66.1% son realmente letales\n   Recall 0.098985           Detecta el 9.9% de los siniestros letales reales\n F1-Score 0.172185                    Balance entre precision y recall: 0.172\n  ROC-AUC 0.812082                         Capacidad de discriminaci√≥n: 0.812\n\n\n\n\nCode\nprint(\"\\nReporte de clasificaci√≥n detallado:\")\nprint(classification_report(y_test, y_pred, target_names=['No Letal', 'Letal']))\n\n\n\nReporte de clasificaci√≥n detallado:\n              precision    recall  f1-score   support\n\n    No Letal       0.93      1.00      0.96      4525\n       Letal       0.66      0.10      0.17       394\n\n    accuracy                           0.92      4919\n   macro avg       0.79      0.55      0.57      4919\nweighted avg       0.91      0.92      0.90      4919\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['No Letal', 'Letal'], \n            yticklabels=['No Letal', 'Letal'])\nplt.title('Matriz de Confusi√≥n - Siniestros Letales', fontsize=14, fontweight='bold')\nplt.xlabel('Predicci√≥n')\nplt.ylabel('Valor Real')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 6))\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nplt.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Tasa de Falsos Positivos (1 - Especificidad)')\nplt.ylabel('Tasa de Verdaderos Positivos (Sensibilidad)')\nplt.title('Curva ROC - Clasificaci√≥n de Siniestros Letales', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.hist(y_pred_proba[y_test==0], bins=40, alpha=0.7, label='No Letal', color='skyblue', density=True)\nplt.hist(y_pred_proba[y_test==1], bins=40, alpha=0.7, label='Letal', color='salmon', density=True)\nplt.xlabel('Probabilidad Predicha de Siniestro Letal')\nplt.ylabel('Densidad')\nplt.title('Distribuci√≥n de Probabilidades Predichas por Clase', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Importancia de features (coeficientes del modelo log√≠stico)\nfeature_names = (numerical_features + \n                list(pipeline.named_steps['preprocessor']\n                    .named_transformers_['cat']\n                    .get_feature_names_out(categorical_features)))\n\ncoefficients = pipeline.named_steps['classifier'].coef_[0]\n\n# Crear DataFrame con importancias\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'coefficient': coefficients,\n    'abs_coefficient': np.abs(coefficients)\n}).sort_values('abs_coefficient', ascending=False)\n\nprint(\"Top 10 features m√°s importantes:\")\nprint(feature_importance.head(10))\n\n\nTop 10 features m√°s importantes:\n                                    feature  coefficient  abs_coefficient\n78                         CANTON_EL CARMEN     2.068902         2.068902\n211                CLASE_CA√çDA DE PASAJEROS    -1.945683         1.945683\n22                       PROVINCIA_COTOPAXI     1.915295         1.915295\n157                     CANTON_PUERTO QUITO     1.656893         1.656893\n75   CANTON_DISTRITO METROPOLITANO DE QUITO    -1.553377         1.553377\n208                        CANTON_ZAPOTILLO     1.536846         1.536846\n175                       CANTON_SAN MIGUEL    -1.486144         1.486144\n216                       CLASE_ROZAMIENTOS    -1.389880         1.389880\n77                         CANTON_ECHEANDIA     1.294437         1.294437\n122                      CANTON_MONTECRISTI    -1.237940         1.237940\n\n\n\n\nCode\n# Visualizar top features\nplt.figure(figsize=(12, 8))\ntop_features = feature_importance.head(15)\ncolors = ['red' if coef &lt; 0 else 'green' for coef in top_features['coefficient']]\nplt.barh(range(len(top_features)), top_features['coefficient'], color=colors)\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Coeficiente')\nplt.title('Top 15 Features m√°s Importantes (Coeficientes del Modelo Log√≠stico)', fontsize=14, fontweight='bold')\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl modelo de clasificaci√≥n binaria muestra los siguientes resultados:\n\nAccuracy (92.4%): Excelente capacidad de clasificaci√≥n general\nROC-AUC (0.81): Excelente capacidad de discriminaci√≥n entre clases\nPrecision (66.1%): Moderada - algunos falsos positivos\nRecall (9.9%): Baja - muchos siniestros letales no detectados\n\n\n\n\nLos factores m√°s importantes para predecir siniestros letales incluyen:\n\nUbicaci√≥n geogr√°fica: Ciertos cantones y provincias muestran mayor riesgo\nTipo de siniestro: Algunas clases de accidentes son m√°s letales que otras\nCausa del accidente: Factores como exceso de velocidad o embriaguez\nZona: Diferencias entre √°reas urbanas y rurales\n\n\n\n\n\nClases desbalanceadas: Solo 8% de siniestros son letales\nRecall bajo: El modelo es conservador, prefiere no clasificar como letal\nAplicaci√≥n pr√°ctica: √ötil para identificar zonas y factores de alto riesgo\n\n\n\n\n\n\nEl modelo identifica patrones geogr√°ficos claros en la letalidad de accidentes\nFactores estructurales (ubicaci√≥n, tipo de v√≠a) son m√°s predictivos que temporales\nAplicaciones en pol√≠tica p√∫blica: Asignaci√≥n de recursos de emergencia y prevenci√≥n\nMejoras futuras: Balanceo de clases y features adicionales podr√≠an mejorar el recall"
  },
  {
    "objectID": "clasificacion_binaria.html#introducci√≥n",
    "href": "clasificacion_binaria.html#introducci√≥n",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Este documento presenta un modelo de clasificaci√≥n binaria para predecir si un siniestro de tr√°nsito ser√° letal (con fallecidos) o no letal, utilizando datos del INEC Ecuador 2019.\nObjetivo: Desarrollar un sistema predictivo que identifique factores de riesgo asociados con la letalidad de accidentes de tr√°nsito."
  },
  {
    "objectID": "clasificacion_binaria.html#carga-y-exploraci√≥n-del-dataset",
    "href": "clasificacion_binaria.html#carga-y-exploraci√≥n-del-dataset",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                           f1_score, confusion_matrix, classification_report, \n                           roc_curve, auc, roc_auc_score)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configurar matplotlib\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"=== SISTEMA DE CLASIFICACI√ìN BINARIA - SINIESTROS LETALES ===\")\n\n\n=== SISTEMA DE CLASIFICACI√ìN BINARIA - SINIESTROS LETALES ===\n\n\n\n\nCode\n# Cargar datos\ndf = pd.read_csv('DATOS_CLASIFICACION_BINARIA/inec_anuario-de-estadisticas-de-transporte_siniestros-de-transito_2019.csv', \n                 sep=';', encoding='utf-8')\n\nprint(f\"Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\nprint(\"\\nPrimeras 5 filas:\")\ndf.head()\n\n\nDatos cargados: 24595 filas, 11 columnas\n\nPrimeras 5 filas:\n\n\n\n\n\n\n\n\n\nMES\nDIA\nHORA\nPROVINCIA\nCANTON\nZONA\nNUM_FALLECIDO\nNUM_LESIONADO\nTOTAL_VICTIMAS\nCLASE\nCAUSA\n\n\n\n\n0\nENERO\nMARTES\n11:00 A 11:59\nIMBABURA\nOTAVALO\nURBANA\n0\n0\n0\nP√âRDIDA DE PISTA\nIMPERICIA E IMPRUDENCIA DEL CONDUCTOR\n\n\n1\nENERO\nMARTES\n23:00 A 23:59\nIMBABURA\nIBARRA\nRURAL\n0\n0\n0\nOTROS\nOTRAS CAUSAS\n\n\n2\nENERO\nMARTES\n12:00 A 12:59\nTUNGURAHUA\nAMBATO\nURBANA\n0\n3\n3\nCHOQUES\nNO RESPETA LAS SE√ëALES DE TR√ÅNSITO\n\n\n3\nENERO\nMARTES\n07:00 A 07:59\nTUNGURAHUA\nAMBATO\nRURAL\n0\n1\n1\nATROPELLOS\nEMBRIAGUEZ O DROGA\n\n\n4\nENERO\nMARTES\n04:00 A 04:59\nPICHINCHA\nDISTRITO METROPOLITANO DE QUITO\nURBANA\n0\n0\n0\nESTRELLAMIENTOS\nEXCESO VELOCIDAD\n\n\n\n\n\n\n\n\n\nCode\n# Exploraci√≥n inicial\nprint(\"=== EXPLORACI√ìN DE DATOS ===\")\nprint(f\"Columnas: {list(df.columns)}\")\nprint(f\"\\nValores faltantes por columna:\")\ndf.isnull().sum()\n\n\n=== EXPLORACI√ìN DE DATOS ===\nColumnas: ['MES', 'DIA', 'HORA', 'PROVINCIA', 'CANTON', 'ZONA', 'NUM_FALLECIDO', 'NUM_LESIONADO', 'TOTAL_VICTIMAS', 'CLASE', 'CAUSA']\n\nValores faltantes por columna:\n\n\nMES               0\nDIA               0\nHORA              0\nPROVINCIA         0\nCANTON            0\nZONA              0\nNUM_FALLECIDO     0\nNUM_LESIONADO     0\nTOTAL_VICTIMAS    0\nCLASE             0\nCAUSA             0\ndtype: int64"
  },
  {
    "objectID": "clasificacion_binaria.html#definici√≥n-de-la-variable-objetivo",
    "href": "clasificacion_binaria.html#definici√≥n-de-la-variable-objetivo",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\n# Crear variable objetivo binaria\nprint(\"=== CREANDO VARIABLE OBJETIVO ===\")\ndf['SINIESTRO_LETAL'] = (df['NUM_FALLECIDO'] &gt; 0).astype(int)\n\nprint(f\"Distribuci√≥n de la variable objetivo:\")\nprint(df['SINIESTRO_LETAL'].value_counts())\nprint(f\"Porcentaje de siniestros letales: {df['SINIESTRO_LETAL'].mean()*100:.2f}%\")\n\n# Visualizar distribuci√≥n\nplt.figure(figsize=(8, 5))\ndf['SINIESTRO_LETAL'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\nplt.title('Distribuci√≥n de Siniestros: Letales vs No Letales')\nplt.xlabel('Tipo de Siniestro')\nplt.ylabel('Cantidad')\nplt.xticks([0, 1], ['No Letal', 'Letal'], rotation=0)\nplt.show()\n\n\n=== CREANDO VARIABLE OBJETIVO ===\nDistribuci√≥n de la variable objetivo:\n0    22626\n1     1969\nName: SINIESTRO_LETAL, dtype: int64\nPorcentaje de siniestros letales: 8.01%"
  },
  {
    "objectID": "clasificacion_binaria.html#preparaci√≥n-de-features",
    "href": "clasificacion_binaria.html#preparaci√≥n-de-features",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\nprint(\"=== PREPARANDO FEATURES ===\")\n# Seleccionar columnas para el modelo\nfeature_cols = ['MES', 'DIA', 'HORA', 'PROVINCIA', 'CANTON', 'ZONA', 'CLASE', 'CAUSA']\nX = df[feature_cols].copy()\ny = df['SINIESTRO_LETAL']\n\nprint(f\"Features seleccionadas: {feature_cols}\")\nprint(f\"Forma de X: {X.shape}\")\n\n# Procesar la columna HORA para extraer la hora num√©rica\ndef extract_hour(hora_str):\n    try:\n        return int(hora_str.split(':')[0])\n    except:\n        return 12  # valor por defecto\n\nX['HORA_NUM'] = X['HORA'].apply(extract_hour)\nX = X.drop('HORA', axis=1)\n\n# Identificar variables categ√≥ricas y num√©ricas\ncategorical_features = ['MES', 'DIA', 'PROVINCIA', 'CANTON', 'ZONA', 'CLASE', 'CAUSA']\nnumerical_features = ['HORA_NUM']\n\nprint(f\"Features categ√≥ricas: {categorical_features}\")\nprint(f\"Features num√©ricas: {numerical_features}\")\n\n\n=== PREPARANDO FEATURES ===\nFeatures seleccionadas: ['MES', 'DIA', 'HORA', 'PROVINCIA', 'CANTON', 'ZONA', 'CLASE', 'CAUSA']\nForma de X: (24595, 8)\nFeatures categ√≥ricas: ['MES', 'DIA', 'PROVINCIA', 'CANTON', 'ZONA', 'CLASE', 'CAUSA']\nFeatures num√©ricas: ['HORA_NUM']"
  },
  {
    "objectID": "clasificacion_binaria.html#divisi√≥n-de-datos",
    "href": "clasificacion_binaria.html#divisi√≥n-de-datos",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\nprint(\"=== DIVIDIENDO DATOS ===\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Entrenamiento: {X_train.shape[0]} muestras\")\nprint(f\"Prueba: {X_test.shape[0]} muestras\")\nprint(f\"Distribuci√≥n en entrenamiento:\")\nprint(y_train.value_counts(normalize=True))\n\n\n=== DIVIDIENDO DATOS ===\nEntrenamiento: 19676 muestras\nPrueba: 4919 muestras\nDistribuci√≥n en entrenamiento:\n0    0.919953\n1    0.080047\nName: SINIESTRO_LETAL, dtype: float64"
  },
  {
    "objectID": "clasificacion_binaria.html#definici√≥n-y-entrenamiento-del-modelo",
    "href": "clasificacion_binaria.html#definici√≥n-y-entrenamiento-del-modelo",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\nprint(\"=== CREANDO PIPELINE ===\")\n# Crear pipeline de preprocesamiento\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n    ])\n\n# Pipeline completo\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n])\n\n# Entrenar modelo\nprint(\"Entrenando modelo LogisticRegression...\")\npipeline.fit(X_train, y_train)\nprint(\"‚úì Modelo entrenado exitosamente\")\n\n\n=== CREANDO PIPELINE ===\nEntrenando modelo LogisticRegression...\n‚úì Modelo entrenado exitosamente"
  },
  {
    "objectID": "clasificacion_binaria.html#generaci√≥n-de-predicciones",
    "href": "clasificacion_binaria.html#generaci√≥n-de-predicciones",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\nprint(\"=== REALIZANDO PREDICCIONES ===\")\ny_pred = pipeline.predict(X_test)\ny_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n\nprint(f\"Predicciones generadas para {len(y_pred)} muestras\")\nprint(f\"Distribuci√≥n de predicciones:\")\nprint(pd.Series(y_pred).value_counts())\n\n\n=== REALIZANDO PREDICCIONES ===\nPredicciones generadas para 4919 muestras\nDistribuci√≥n de predicciones:\n0    4860\n1      59\ndtype: int64"
  },
  {
    "objectID": "clasificacion_binaria.html#evaluaci√≥n-del-modelo",
    "href": "clasificacion_binaria.html#evaluaci√≥n-del-modelo",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\nprint(\"=== EVALUACI√ìN DEL MODELO ===\")\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Crear DataFrame con m√©tricas\nmetricas = pd.DataFrame({\n    'M√©trica': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n    'Valor': [accuracy, precision, recall, f1, roc_auc],\n    'Interpretaci√≥n': [\n        f'Clasifica correctamente el {accuracy*100:.1f}% de los casos',\n        f'De los predichos como letales, {precision*100:.1f}% son realmente letales',\n        f'Detecta el {recall*100:.1f}% de los siniestros letales reales',\n        f'Balance entre precision y recall: {f1:.3f}',\n        f'Capacidad de discriminaci√≥n: {roc_auc:.3f}'\n    ]\n})\n\nprint(metricas.to_string(index=False))\n\n\n=== EVALUACI√ìN DEL MODELO ===\n  M√©trica    Valor                                             Interpretaci√≥n\n Accuracy 0.923765              Clasifica correctamente el 92.4% de los casos\nPrecision 0.661017 De los predichos como letales, 66.1% son realmente letales\n   Recall 0.098985           Detecta el 9.9% de los siniestros letales reales\n F1-Score 0.172185                    Balance entre precision y recall: 0.172\n  ROC-AUC 0.812082                         Capacidad de discriminaci√≥n: 0.812\n\n\n\n\nCode\nprint(\"\\nReporte de clasificaci√≥n detallado:\")\nprint(classification_report(y_test, y_pred, target_names=['No Letal', 'Letal']))\n\n\n\nReporte de clasificaci√≥n detallado:\n              precision    recall  f1-score   support\n\n    No Letal       0.93      1.00      0.96      4525\n       Letal       0.66      0.10      0.17       394\n\n    accuracy                           0.92      4919\n   macro avg       0.79      0.55      0.57      4919\nweighted avg       0.91      0.92      0.90      4919"
  },
  {
    "objectID": "clasificacion_binaria.html#visualizaciones",
    "href": "clasificacion_binaria.html#visualizaciones",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['No Letal', 'Letal'], \n            yticklabels=['No Letal', 'Letal'])\nplt.title('Matriz de Confusi√≥n - Siniestros Letales', fontsize=14, fontweight='bold')\nplt.xlabel('Predicci√≥n')\nplt.ylabel('Valor Real')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 6))\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nplt.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Tasa de Falsos Positivos (1 - Especificidad)')\nplt.ylabel('Tasa de Verdaderos Positivos (Sensibilidad)')\nplt.title('Curva ROC - Clasificaci√≥n de Siniestros Letales', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.hist(y_pred_proba[y_test==0], bins=40, alpha=0.7, label='No Letal', color='skyblue', density=True)\nplt.hist(y_pred_proba[y_test==1], bins=40, alpha=0.7, label='Letal', color='salmon', density=True)\nplt.xlabel('Probabilidad Predicha de Siniestro Letal')\nplt.ylabel('Densidad')\nplt.title('Distribuci√≥n de Probabilidades Predichas por Clase', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "clasificacion_binaria.html#an√°lisis-de-importancia-de-features",
    "href": "clasificacion_binaria.html#an√°lisis-de-importancia-de-features",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "Code\n# Importancia de features (coeficientes del modelo log√≠stico)\nfeature_names = (numerical_features + \n                list(pipeline.named_steps['preprocessor']\n                    .named_transformers_['cat']\n                    .get_feature_names_out(categorical_features)))\n\ncoefficients = pipeline.named_steps['classifier'].coef_[0]\n\n# Crear DataFrame con importancias\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'coefficient': coefficients,\n    'abs_coefficient': np.abs(coefficients)\n}).sort_values('abs_coefficient', ascending=False)\n\nprint(\"Top 10 features m√°s importantes:\")\nprint(feature_importance.head(10))\n\n\nTop 10 features m√°s importantes:\n                                    feature  coefficient  abs_coefficient\n78                         CANTON_EL CARMEN     2.068902         2.068902\n211                CLASE_CA√çDA DE PASAJEROS    -1.945683         1.945683\n22                       PROVINCIA_COTOPAXI     1.915295         1.915295\n157                     CANTON_PUERTO QUITO     1.656893         1.656893\n75   CANTON_DISTRITO METROPOLITANO DE QUITO    -1.553377         1.553377\n208                        CANTON_ZAPOTILLO     1.536846         1.536846\n175                       CANTON_SAN MIGUEL    -1.486144         1.486144\n216                       CLASE_ROZAMIENTOS    -1.389880         1.389880\n77                         CANTON_ECHEANDIA     1.294437         1.294437\n122                      CANTON_MONTECRISTI    -1.237940         1.237940\n\n\n\n\nCode\n# Visualizar top features\nplt.figure(figsize=(12, 8))\ntop_features = feature_importance.head(15)\ncolors = ['red' if coef &lt; 0 else 'green' for coef in top_features['coefficient']]\nplt.barh(range(len(top_features)), top_features['coefficient'], color=colors)\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Coeficiente')\nplt.title('Top 15 Features m√°s Importantes (Coeficientes del Modelo Log√≠stico)', fontsize=14, fontweight='bold')\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "clasificacion_binaria.html#interpretaci√≥n-de-resultados",
    "href": "clasificacion_binaria.html#interpretaci√≥n-de-resultados",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "El modelo de clasificaci√≥n binaria muestra los siguientes resultados:\n\nAccuracy (92.4%): Excelente capacidad de clasificaci√≥n general\nROC-AUC (0.81): Excelente capacidad de discriminaci√≥n entre clases\nPrecision (66.1%): Moderada - algunos falsos positivos\nRecall (9.9%): Baja - muchos siniestros letales no detectados\n\n\n\n\nLos factores m√°s importantes para predecir siniestros letales incluyen:\n\nUbicaci√≥n geogr√°fica: Ciertos cantones y provincias muestran mayor riesgo\nTipo de siniestro: Algunas clases de accidentes son m√°s letales que otras\nCausa del accidente: Factores como exceso de velocidad o embriaguez\nZona: Diferencias entre √°reas urbanas y rurales\n\n\n\n\n\nClases desbalanceadas: Solo 8% de siniestros son letales\nRecall bajo: El modelo es conservador, prefiere no clasificar como letal\nAplicaci√≥n pr√°ctica: √ötil para identificar zonas y factores de alto riesgo"
  },
  {
    "objectID": "clasificacion_binaria.html#conclusiones",
    "href": "clasificacion_binaria.html#conclusiones",
    "title": "Clasificaci√≥n Binaria - Siniestros de Tr√°nsito Letales",
    "section": "",
    "text": "El modelo identifica patrones geogr√°ficos claros en la letalidad de accidentes\nFactores estructurales (ubicaci√≥n, tipo de v√≠a) son m√°s predictivos que temporales\nAplicaciones en pol√≠tica p√∫blica: Asignaci√≥n de recursos de emergencia y prevenci√≥n\nMejoras futuras: Balanceo de clases y features adicionales podr√≠an mejorar el recall"
  },
  {
    "objectID": "ml_text.html",
    "href": "ml_text.html",
    "title": "Cargar datos",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport requests\nimport nltk\n# downloading some additional packages and corpora\nnltk.download('punkt', quiet=True) # necessary for tokenization\nnltk.download('punkt_tab', quiet=True) # necessary for tokenization\nnltk.download('wordnet', quiet=True) # necessary for lemmatization\nnltk.download('stopwords', quiet=True) # necessary for removal of stop words\nnltk.download('averaged_perceptron_tagger', quiet=True) # necessary for POS tagging\nnltk.download('averaged_perceptron_tagger_eng', quiet=True) # necessary for POS tagging\nnltk.download('maxent_ne_chunker', quiet=True) # necessary for entity extraction\nnltk.download('omw-1.4', quiet=True) # necessary for lemmatization\nnltk.download('words', quiet=True)\nprint(\"NLTK resources downloaded successfully\")\n\n\nNLTK resources downloaded successfully\n\n\n\n\nCode\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/master/story.txt\"\n\nr = requests.get(url)\n\nr.encoding = 'utf-8'\nstory = r.text\nprint(story[:20])\nprint('este fue el texto')\n\n\nThe seventh Sally or\neste fue el texto\n\n\n\nTokenizaci√≥n\n\n\nCode\nfrom nltk import word_tokenize,pos_tag\n\nwords = word_tokenize(story)\nwords[:20]\n\n\n['The',\n 'seventh',\n 'Sally',\n 'or',\n 'how',\n 'Trurl',\n \"'s\",\n 'own',\n 'perfection',\n 'led',\n 'to',\n 'no',\n 'good',\n 'By',\n 'Stanis√Ö‚Äöaw',\n 'Lem',\n ',',\n '1965',\n '.',\n 'Translated']\n\n\n\n\nStriming and Lemmatizaci√≥n\n\n\nCode\nfrom nltk.stem import PorterStemmer as stemmer\nfrom nltk.stem import WordNetLemmatizer as lemmatizer\nfrom nltk.corpus import wordnet\n\npalabra = 'changing'\nprint(\"palabra:\",palabra)\n#steaming\nprint(\"stemming:\",stemmer().stem(palabra))\n\n#lemmatizaci√≥n\nprint(\"lemmatizaci√≥n:\",lemmatizer().lemmatize(palabra,pos= wordnet.VERB))\n\n\npalabra: changing\nstemming: chang\nlemmatizaci√≥n: change\n\n\n\n\nPART OF SPEECH - POS TAG\n\n\nCode\nfrom nltk import pos_tag\npos_tag(words[:20])\n\n\n[('The', 'DT'),\n ('seventh', 'JJ'),\n ('Sally', 'NNP'),\n ('or', 'CC'),\n ('how', 'WRB'),\n ('Trurl', 'NNP'),\n (\"'s\", 'POS'),\n ('own', 'JJ'),\n ('perfection', 'NN'),\n ('led', 'VBD'),\n ('to', 'TO'),\n ('no', 'DT'),\n ('good', 'JJ'),\n ('By', 'IN'),\n ('Stanis√Ö‚Äöaw', 'NNP'),\n ('Lem', 'NNP'),\n (',', ','),\n ('1965', 'CD'),\n ('.', '.'),\n ('Translated', 'VBN')]\n\n\n\n\nStop words\n\n\nCode\nfrom nltk.corpus import stopwords as stop\nstopwords = stop.words('english')\nstopwords[:20]\n\n#fin de script\n\n\n['a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been']\n\n\n\n\nstop words in story\n\n\nCode\ntokens = nltk.word_tokenize(story.lower())\n\n#limpieza de nuemero\nlettertokens = [token for token in tokens if token.isalpha()]\n\nwithout_stopwords = [token for token in lettertokens if token not in stopwords]\nwithout_stopwords[:20]\n\n#fin de script\n\n\n['seventh',\n 'sally',\n 'trurl',\n 'perfection',\n 'led',\n 'good',\n 'lem',\n 'translated',\n 'michael',\n 'kandel',\n 'universe',\n 'infinite',\n 'bounded',\n 'therefore',\n 'beam',\n 'light',\n 'whatever',\n 'direction',\n 'may',\n 'travel']"
  },
  {
    "objectID": "text_classification.html",
    "href": "text_classification.html",
    "title": "Text Classification",
    "section": "",
    "text": "Importar librer√≠as\n\nfrom sklearn.feature_extraction.text import CountVectorizer #BoW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.datasets import fetch_20newsgroups\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize # tokenizacion\nfrom nltk import pos_tag #lematizacion\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#stop words\nlemmatizer = WordNetLemmatizer()\n\nnltk.download('stopwords') # necessary for removal of stop words\nnltk.download('wordnet') # necessary for lemmatization\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/juanchx/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/juanchx/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\n\ncargar dataset\n\ncategories = ['comp.graphics','comp.sys.mac.hardware','rec.sport.baseball','talk.politics.misc']\nnewsgroups = fetch_20newsgroups(subset='train',categories=categories,remove=('headers','footers','quotes'))\nprint(newsgroups.target_names)\n\n#fin\n\n['comp.graphics', 'comp.sys.mac.hardware', 'rec.sport.baseball', 'talk.politics.misc']\n\n\n\n\nFeatures and target\n\nX = newsgroups.data\ny = newsgroups.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nstop_words = stopwords.words('english')\nvectoizer = TfidfVectorizer(stop_words=stop_words)\nvectoizer = CountVectorizer(stop_words=stop_words)\n\nnaive_bayes = MultinomialNB()\n#pipelline \npipe = Pipeline([\n      (\"vectorizacion\",vectoizer),\n      (\"naive_bayes\",naive_bayes)\n])\n\n#end script\n\n#entrenamiento\n\npipe.fit(X_train,y_train)\n\nPipeline(steps=[('vectorizacion',\n                 CountVectorizer(stop_words=['a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...])),\n                ('naive_bayes', MultinomialNB())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('vectorizacion',\n                 CountVectorizer(stop_words=['a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...])),\n                ('naive_bayes', MultinomialNB())])CountVectorizerCountVectorizer(stop_words=['a', 'about', 'above', 'after', 'again', 'against',\n                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n                            'been', 'before', 'being', 'below', 'between',\n                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...])MultinomialNBMultinomialNB()\n\n\n#predicci√≥n\n\ny_pred = pipe.predict(X_test)\n\n#reporte\n\n#track the name of the categories\nprint(classification_report(y_test,y_pred,target_names=newsgroups.target_names))\n\n                       precision    recall  f1-score   support\n\n        comp.graphics       0.92      0.91      0.91       117\ncomp.sys.mac.hardware       0.91      0.92      0.91       116\n   rec.sport.baseball       0.90      0.90      0.90       119\n   talk.politics.misc       0.89      0.89      0.89        93\n\n             accuracy                           0.91       445\n            macro avg       0.91      0.91      0.90       445\n         weighted avg       0.91      0.91      0.91       445\n\n\n\n\n\nMatriz de confusi√≥n\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred),display_labels=newsgroups.target_names)\ndisp.plot()\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Landos.html",
    "href": "Landos.html",
    "title": "Clasificaci√≥n de Tweets - Predicci√≥n de Popularidad",
    "section": "",
    "text": "Predicci√≥n del nivel de popularidad de usuarios de Twitter bas√°ndose en el contenido de sus tweets.\nHip√≥tesis: El estilo de escritura revela informaci√≥n sobre la popularidad del autor."
  },
  {
    "objectID": "Landos.html#resultados",
    "href": "Landos.html#resultados",
    "title": "Clasificaci√≥n de Tweets - Predicci√≥n de Popularidad",
    "section": "Resultados",
    "text": "Resultados\n\nAccuracy Test: ~56% (supera baseline 33.3%)\nMejor predicha: BAJA (recall ~0.81)\nM√°s dif√≠cil: MEDIA (recall ~0.26)"
  },
  {
    "objectID": "Landos.html#interpretaci√≥n",
    "href": "Landos.html#interpretaci√≥n",
    "title": "Clasificaci√≥n de Tweets - Predicci√≥n de Popularidad",
    "section": "Interpretaci√≥n",
    "text": "Interpretaci√≥n\nEl estilo de escritura S√ç contiene informaci√≥n sobre popularidad:\n\nUsuarios con BAJA popularidad: patrones distintivos\nClase MEDIA: dif√≠cil de diferenciar\nUsuarios ALTA: moderadamente distinguibles"
  },
  {
    "objectID": "Landos.html#aplicaciones",
    "href": "Landos.html#aplicaciones",
    "title": "Clasificaci√≥n de Tweets - Predicci√≥n de Popularidad",
    "section": "Aplicaciones",
    "text": "Aplicaciones\n\nIdentificar cuentas influyentes por contenido\nDetectar patrones de popularidad\nSegmentaci√≥n de usuarios"
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Clasificaci√≥n de Tweets: Predicci√≥n de Popularidad de Usuarios",
    "section": "",
    "text": "Introducci√≥n\nEste an√°lisis tiene como objetivo predecir el nivel de popularidad de usuarios de Twitter bas√°ndose en el contenido de sus tweets y caracter√≠sticas asociadas. Utilizamos t√©cnicas de Machine Learning para clasificaci√≥n de texto y an√°lisis de patrones de comportamiento.\nHip√≥tesis: El estilo de escritura y las caracter√≠sticas de los tweets pueden revelar informaci√≥n sobre la popularidad del autor (medida por n√∫mero de seguidores).\n\n\n1. Carga y Exploraci√≥n del Dataset\n#| label: setup #| echo: true\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import re from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import ( classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_recall_fscore_support )\n\n\nConfiguraci√≥n de visualizaci√≥n\nplt.style.use(‚Äòseaborn-v0_8-darkgrid‚Äô) sns.set_palette(‚Äúhusl‚Äù)\n\n\nCargar dataset\ndf = pd.read_csv(‚Äòdata_twwet/tweets_totales_con_sentimiento_ml.csv‚Äô)\nprint(f‚ÄùTotal de registros: {len(df):,}‚Äú) print(f‚ÄùAutores √∫nicos: {df[‚ÄòauthorId‚Äô].nunique():,}‚Äú) print(f‚ÄùPeriodo: {df[‚ÄòcreatedAt‚Äô].min()} a {df[‚ÄòcreatedAt‚Äô].max()}‚Äú)"
  }
]